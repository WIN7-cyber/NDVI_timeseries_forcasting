{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture Overview\n",
    "\n",
    "The proposed model is based on a ConvLSTM architecture designed for spatiotemporal sequence forecasting.\n",
    "Given a sequence of historical inputs, the model simultaneously predicts multiple future time steps using a multi-output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Parameters and Their Functions\n",
    "1. filters = 16\n",
    "\n",
    "The parameter filters specifies the number of convolutional kernels in the ConvLSTM layer, which corresponds to the number of feature maps in the hidden state.\n",
    "\n",
    "A larger number of filters increases the model’s capacity to learn complex spatiotemporal patterns.\n",
    "\n",
    "However, excessively large values may lead to overfitting, especially when the dataset size is limited.\n",
    "\n",
    "In this work, filters = 16 is chosen as a balanced trade-off between representation capability and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. kernel_size = (3, 3)\n",
    "\n",
    "The kernel_size determines the spatial receptive field of the convolution operation.\n",
    "\n",
    "A 3×3 kernel allows the model to capture local spatial dependencies while maintaining computational efficiency.\n",
    "\n",
    "This choice is commonly adopted in spatiotemporal modeling to balance detail preservation and smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. lookback (input sequence length)\n",
    "\n",
    "The lookback window defines how many past time steps are used as input for prediction.\n",
    "\n",
    "A longer lookback provides richer temporal context.\n",
    "\n",
    "However, overly long sequences may introduce noise and increase the risk of overfitting.\n",
    "\n",
    "The selected lookback length is sufficient to capture temporal dynamics while preserving training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. horizon = 7\n",
    "\n",
    "The forecast horizon indicates the number of future time steps predicted by the model.\n",
    "\n",
    "Instead of recursive forecasting, the model adopts a direct multi-step prediction strategy.\n",
    "\n",
    "The output layer simultaneously predicts values from t+1 to t+7, reducing error accumulation across time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Dense(horizon)\n",
    "\n",
    "The fully connected output layer maps the shared spatiotemporal representation to multiple future targets.\n",
    "\n",
    "Each output neuron corresponds to a specific forecast lead time.\n",
    "\n",
    "All forecast steps are generated in parallel, rather than sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Loss Function (e.g., Mean Squared Error)\n",
    "\n",
    "The loss function evaluates the discrepancy between predicted and observed values across all forecast steps.\n",
    "\n",
    "The final loss is computed as the average error over all prediction horizons.\n",
    "\n",
    "This encourages the model to learn both short-term and long-term predictive patterns.\n",
    "\n",
    "7. Early Stopping and Validation Strategy\n",
    "\n",
    "Early stopping is applied based on validation loss to prevent overfitting.\n",
    "\n",
    "Training terminates when validation performance no longer improves.\n",
    "\n",
    "This strategy ensures better generalization to unseen future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个文件是“多步预测（multi-step）”，同时输出 t+1 到 t+7，虽然不同预测步长（t+1 与 t+7）的结果在图像中分别展示，但两者均来源于同一 ConvLSTM 模型的输出；\n",
    "结果差异仅源于滑动窗口下预测起点 t0 的不同，而非模型结构或参数的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进 ： 改成小 window，filters = 8 or 16, Dropout(0.2)。在训练时，随机“关闭”一部分神经元，防止模型过度依赖某些特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region=Grandvillers_Sec | probes=['Sec'] (count=1)\n",
      "[INFO] Grandvillers_Sec/Sec window=14 | rows=115 | samples train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.8525 - val_loss: 0.7692 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1920 - val_loss: 0.7716 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9257 - val_loss: 0.7715 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8256 - val_loss: 0.7713 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6853 - val_loss: 0.7738 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5623\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6184 - val_loss: 0.7788 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5806 - val_loss: 0.7843 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4972 - val_loss: 0.7893 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5565 - val_loss: 0.7914 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4881 - val_loss: 0.7966 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4181\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4741 - val_loss: 0.8026 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_14\\Grandvillers_Sec\\Sec\n",
      "[INFO] Grandvillers_Sec/Sec window=30 | rows=115 | samples train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 1.9875 - val_loss: 0.6848 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.3589 - val_loss: 0.7013 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.9997 - val_loss: 0.7115 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.9084 - val_loss: 0.7128 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.9076 - val_loss: 0.7142 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5737\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6862 - val_loss: 0.7155 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6937 - val_loss: 0.7149 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5388 - val_loss: 0.7138 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5717 - val_loss: 0.7131 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5131 - val_loss: 0.7131 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5669\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5924 - val_loss: 0.7137 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_30\\Grandvillers_Sec\\Sec\n",
      "\n",
      "Region=Grandvillers_Canon | probes=['Canon1', 'Canon2'] (count=2)\n",
      "[INFO] Grandvillers_Canon/Canon1 window=14 | rows=116 | samples train=61, val=11, test=12\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 1.8610 - val_loss: 0.4484 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.5276 - val_loss: 0.4574 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1829 - val_loss: 0.4624 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0673 - val_loss: 0.4678 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0309 - val_loss: 0.4741 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.9905\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8795 - val_loss: 0.4799 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9518 - val_loss: 0.4829 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8611 - val_loss: 0.4852 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7391 - val_loss: 0.4877 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7072 - val_loss: 0.4908 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7034\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6571 - val_loss: 0.4948 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_14\\Grandvillers_Canon\\Canon1\n",
      "[INFO] Grandvillers_Canon/Canon1 window=30 | rows=116 | samples train=45, val=11, test=12\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 3.2385 - val_loss: 0.3756 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.8885 - val_loss: 0.3841 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.6182 - val_loss: 0.3928 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.0457 - val_loss: 0.3994 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.9301 - val_loss: 0.4047 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.9028\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.9206 - val_loss: 0.4088 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8029 - val_loss: 0.4102 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7383 - val_loss: 0.4117 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7557 - val_loss: 0.4133 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8157 - val_loss: 0.4146 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.7257\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7111 - val_loss: 0.4160 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_30\\Grandvillers_Canon\\Canon1\n",
      "[INFO] Grandvillers_Canon/Canon2 window=14 | rows=115 | samples train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.6062 - val_loss: 0.3901 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.2475 - val_loss: 0.3924 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0406 - val_loss: 0.3947 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9776 - val_loss: 0.3937 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9432 - val_loss: 0.3956 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.8374\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7971 - val_loss: 0.3987 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7243 - val_loss: 0.4002 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6780 - val_loss: 0.4010 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7059 - val_loss: 0.4027 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7010 - val_loss: 0.4058 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6797\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7394 - val_loss: 0.4085 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_14\\Grandvillers_Canon\\Canon2\n",
      "[INFO] Grandvillers_Canon/Canon2 window=30 | rows=115 | samples train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 2.5623 - val_loss: 0.5306 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.7781 - val_loss: 0.5284 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.4267 - val_loss: 0.5231 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.3166 - val_loss: 0.5088 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2697 - val_loss: 0.4919 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0355 - val_loss: 0.4756 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8436 - val_loss: 0.4653 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7645 - val_loss: 0.4606 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7386 - val_loss: 0.4603 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6095 - val_loss: 0.4625 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7317 - val_loss: 0.4668 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6244 - val_loss: 0.4713 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7723 - val_loss: 0.4766 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.7259\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6732 - val_loss: 0.4814 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5558 - val_loss: 0.4831 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5873 - val_loss: 0.4847 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6116 - val_loss: 0.4857 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5524 - val_loss: 0.4868 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4802\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5989 - val_loss: 0.4869 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_30\\Grandvillers_Canon\\Canon2\n",
      "\n",
      "Region=Grandvillers_Robot_20 | probes=['Robot-20'] (count=1)\n",
      "[INFO] Grandvillers_Robot_20/Robot-20 window=14 | rows=113 | samples train=59, val=10, test=12\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 2.2933 - val_loss: 1.3564 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.6186 - val_loss: 1.3684 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1468 - val_loss: 1.3762 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1308 - val_loss: 1.3792 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9446 - val_loss: 1.3753 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.8813\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8029 - val_loss: 1.3716 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7542 - val_loss: 1.3697 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7217 - val_loss: 1.3664 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7293 - val_loss: 1.3616 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6703 - val_loss: 1.3581 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7349\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6570 - val_loss: 1.3563 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5892 - val_loss: 1.3564 - lr: 2.5000e-04\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5747 - val_loss: 1.3570 - lr: 2.5000e-04\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5402 - val_loss: 1.3576 - lr: 2.5000e-04\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6394 - val_loss: 1.3578 - lr: 2.5000e-04\n",
      "Epoch 16/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7291\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6335 - val_loss: 1.3592 - lr: 2.5000e-04\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6224 - val_loss: 1.3600 - lr: 1.2500e-04\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5708 - val_loss: 1.3603 - lr: 1.2500e-04\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6047 - val_loss: 1.3598 - lr: 1.2500e-04\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5953 - val_loss: 1.3597 - lr: 1.2500e-04\n",
      "Epoch 21/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6238\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6202 - val_loss: 1.3594 - lr: 1.2500e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_14\\Grandvillers_Robot_20\\Robot-20\n",
      "[INFO] Grandvillers_Robot_20/Robot-20 window=30 | rows=113 | samples train=43, val=10, test=12\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 1.8847 - val_loss: 1.3602 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.3084 - val_loss: 1.3844 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.9733 - val_loss: 1.4008 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7885 - val_loss: 1.4165 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7375 - val_loss: 1.4295 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6285\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5845 - val_loss: 1.4411 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5738 - val_loss: 1.4446 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5919 - val_loss: 1.4442 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5732 - val_loss: 1.4431 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5557 - val_loss: 1.4409 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5008\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4898 - val_loss: 1.4389 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_30\\Grandvillers_Robot_20\\Robot-20\n",
      "\n",
      "Region=Grandvillers_Robot | probes=['Canon3', 'Robot'] (count=2)\n",
      "[INFO] Grandvillers_Robot/Canon3 window=14 | rows=115 | samples train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 2.4950 - val_loss: 0.5896 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.6639 - val_loss: 0.6023 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 1.4181 - val_loss: 0.6051 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0620 - val_loss: 0.6047 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0335 - val_loss: 0.6028 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.8578\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8497 - val_loss: 0.5998 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8454 - val_loss: 0.5952 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7170 - val_loss: 0.5909 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6859 - val_loss: 0.5873 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6628 - val_loss: 0.5861 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7491 - val_loss: 0.5840 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6562 - val_loss: 0.5829 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6649 - val_loss: 0.5822 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6099 - val_loss: 0.5817 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6002 - val_loss: 0.5812 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5607 - val_loss: 0.5801 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6600 - val_loss: 0.5783 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6096 - val_loss: 0.5765 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6011 - val_loss: 0.5722 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4979 - val_loss: 0.5692 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5715 - val_loss: 0.5687 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5816 - val_loss: 0.5692 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5283 - val_loss: 0.5698 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5278 - val_loss: 0.5704 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4669 - val_loss: 0.5686 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5297 - val_loss: 0.5670 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5148 - val_loss: 0.5633 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5215 - val_loss: 0.5597 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4891 - val_loss: 0.5580 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4401 - val_loss: 0.5565 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4193 - val_loss: 0.5539 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4790 - val_loss: 0.5502 - lr: 5.0000e-04\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4322 - val_loss: 0.5473 - lr: 5.0000e-04\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4477 - val_loss: 0.5423 - lr: 5.0000e-04\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4207 - val_loss: 0.5393 - lr: 5.0000e-04\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4009 - val_loss: 0.5368 - lr: 5.0000e-04\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3992 - val_loss: 0.5350 - lr: 5.0000e-04\n",
      "Epoch 38/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4037 - val_loss: 0.5343 - lr: 5.0000e-04\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4081 - val_loss: 0.5366 - lr: 5.0000e-04\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4362 - val_loss: 0.5331 - lr: 5.0000e-04\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4046 - val_loss: 0.5252 - lr: 5.0000e-04\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4127 - val_loss: 0.5197 - lr: 5.0000e-04\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4248 - val_loss: 0.5158 - lr: 5.0000e-04\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3857 - val_loss: 0.5133 - lr: 5.0000e-04\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4131 - val_loss: 0.5164 - lr: 5.0000e-04\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3678 - val_loss: 0.5163 - lr: 5.0000e-04\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3952 - val_loss: 0.5160 - lr: 5.0000e-04\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3670 - val_loss: 0.5161 - lr: 5.0000e-04\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3713 - val_loss: 0.5118 - lr: 5.0000e-04\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4381 - val_loss: 0.5067 - lr: 5.0000e-04\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4099 - val_loss: 0.5057 - lr: 5.0000e-04\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3812 - val_loss: 0.5047 - lr: 5.0000e-04\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3664 - val_loss: 0.5045 - lr: 5.0000e-04\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3616 - val_loss: 0.5041 - lr: 5.0000e-04\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3289 - val_loss: 0.5033 - lr: 5.0000e-04\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3755 - val_loss: 0.5032 - lr: 5.0000e-04\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3750 - val_loss: 0.5070 - lr: 5.0000e-04\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3881 - val_loss: 0.5131 - lr: 5.0000e-04\n",
      "Epoch 59/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3673 - val_loss: 0.5178 - lr: 5.0000e-04\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3559 - val_loss: 0.5152 - lr: 5.0000e-04\n",
      "Epoch 61/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3254\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2938 - val_loss: 0.5095 - lr: 5.0000e-04\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3111 - val_loss: 0.5078 - lr: 2.5000e-04\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3549 - val_loss: 0.5077 - lr: 2.5000e-04\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3311 - val_loss: 0.5058 - lr: 2.5000e-04\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3444 - val_loss: 0.5049 - lr: 2.5000e-04\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3344 - val_loss: 0.5029 - lr: 2.5000e-04\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3739 - val_loss: 0.4985 - lr: 2.5000e-04\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3337 - val_loss: 0.4946 - lr: 2.5000e-04\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3048 - val_loss: 0.4914 - lr: 2.5000e-04\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3461 - val_loss: 0.4861 - lr: 2.5000e-04\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3440 - val_loss: 0.4848 - lr: 2.5000e-04\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3356 - val_loss: 0.4832 - lr: 2.5000e-04\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3205 - val_loss: 0.4827 - lr: 2.5000e-04\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3595 - val_loss: 0.4832 - lr: 2.5000e-04\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3213 - val_loss: 0.4849 - lr: 2.5000e-04\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3370 - val_loss: 0.4848 - lr: 2.5000e-04\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3574 - val_loss: 0.4824 - lr: 2.5000e-04\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3156 - val_loss: 0.4841 - lr: 2.5000e-04\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3263 - val_loss: 0.4864 - lr: 2.5000e-04\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3261 - val_loss: 0.4859 - lr: 2.5000e-04\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2984 - val_loss: 0.4853 - lr: 2.5000e-04\n",
      "Epoch 82/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4984\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4385 - val_loss: 0.4860 - lr: 2.5000e-04\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3561 - val_loss: 0.4863 - lr: 1.2500e-04\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3376 - val_loss: 0.4854 - lr: 1.2500e-04\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3551 - val_loss: 0.4837 - lr: 1.2500e-04\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2989 - val_loss: 0.4806 - lr: 1.2500e-04\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2961 - val_loss: 0.4791 - lr: 1.2500e-04\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3353 - val_loss: 0.4781 - lr: 1.2500e-04\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3184 - val_loss: 0.4789 - lr: 1.2500e-04\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2947 - val_loss: 0.4806 - lr: 1.2500e-04\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3028 - val_loss: 0.4817 - lr: 1.2500e-04\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3730 - val_loss: 0.4812 - lr: 1.2500e-04\n",
      "Epoch 93/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2923\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3312 - val_loss: 0.4803 - lr: 1.2500e-04\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3029 - val_loss: 0.4800 - lr: 6.2500e-05\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2888 - val_loss: 0.4815 - lr: 6.2500e-05\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3045 - val_loss: 0.4832 - lr: 6.2500e-05\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2825 - val_loss: 0.4850 - lr: 6.2500e-05\n",
      "Epoch 98/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3325\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3200 - val_loss: 0.4865 - lr: 6.2500e-05\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_14\\Grandvillers_Robot\\Canon3\n",
      "[INFO] Grandvillers_Robot/Canon3 window=30 | rows=115 | samples train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 2.3652 - val_loss: 0.7008 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.6985 - val_loss: 0.7081 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5630 - val_loss: 0.7094 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1775 - val_loss: 0.7073 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.0369 - val_loss: 0.7014 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.0719 - val_loss: 0.6969 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8414 - val_loss: 0.6933 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8701 - val_loss: 0.6915 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8542 - val_loss: 0.6881 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7455 - val_loss: 0.6867 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7698 - val_loss: 0.6836 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6517 - val_loss: 0.6811 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5965 - val_loss: 0.6803 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6042 - val_loss: 0.6786 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6820 - val_loss: 0.6763 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5065 - val_loss: 0.6770 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5158 - val_loss: 0.6778 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4463 - val_loss: 0.6789 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5159 - val_loss: 0.6813 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5219\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5016 - val_loss: 0.6838 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4305 - val_loss: 0.6835 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4570 - val_loss: 0.6836 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4614 - val_loss: 0.6828 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4502 - val_loss: 0.6812 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5914\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5040 - val_loss: 0.6784 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_30\\Grandvillers_Robot\\Canon3\n",
      "[INFO] Grandvillers_Robot/Robot window=14 | rows=115 | samples train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 2.0971 - val_loss: 0.4306 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.4540 - val_loss: 0.4376 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0220 - val_loss: 0.4425 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9335 - val_loss: 0.4428 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8763 - val_loss: 0.4455 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.9653\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8714 - val_loss: 0.4461 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9052 - val_loss: 0.4448 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8176 - val_loss: 0.4446 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9108 - val_loss: 0.4456 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7934 - val_loss: 0.4461 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7577\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6714 - val_loss: 0.4471 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_14\\Grandvillers_Robot\\Robot\n",
      "[INFO] Grandvillers_Robot/Robot window=30 | rows=115 | samples train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 2.1774 - val_loss: 0.4531 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5176 - val_loss: 0.4561 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.3575 - val_loss: 0.4635 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1726 - val_loss: 0.4730 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1020 - val_loss: 0.4828 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.7602\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.9147 - val_loss: 0.4897 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.9597 - val_loss: 0.4930 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8716 - val_loss: 0.4963 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8099 - val_loss: 0.5002 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8770 - val_loss: 0.5038 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5901\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6921 - val_loss: 0.5070 - lr: 5.0000e-04\n",
      "[OK] Saved outputs -> outputs_multistep_context_smallmodel\\window_30\\Grandvillers_Robot\\Robot\n",
      "\n",
      "All unique probe_name values across 4 CSV:\n",
      "['Canon1', 'Canon2', 'Canon3', 'Robot', 'Robot-20', 'Sec']\n",
      "Total unique probe_name: 6\n",
      "\n",
      "Done. Outputs saved under: outputs_multistep_context_smallmodel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # CSV paths: key=region name, value=absolute path\n",
    "    csv_paths: Dict[str, str] = None\n",
    "\n",
    "    # Column names\n",
    "    date_col: str = \"date\"\n",
    "    probe_col: str = \"probe_name\"\n",
    "\n",
    "    # Feature columns (8 variables)\n",
    "    feature_cols: List[str] = None\n",
    "    target_col: str = \"sm_30cm\"\n",
    "\n",
    "    # Time-series settings\n",
    "    window_list: List[int] = None\n",
    "    horizon: int = 7\n",
    "    split_ratio: Tuple[float, float, float] = (0.70, 0.15, 0.15)\n",
    "\n",
    "    # Model hyperparameters (small model to reduce overfitting)\n",
    "    filters: int = 16          # try 8 if still overfitting\n",
    "    dropout: float = 0.2\n",
    "    kernel_size: Tuple[int, int] = (1, 3)\n",
    "    batch_size: int = 16\n",
    "    max_epochs: int = 150\n",
    "    learning_rate: float = 1e-3\n",
    "\n",
    "    # Output root folder\n",
    "    output_root: str = \"outputs_multistep_context_smallmodel\"\n",
    "\n",
    "    # Reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Data utilities\n",
    "# ============================================================\n",
    "def load_and_clean_csv(path: str, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV, parse date, keep required columns, and drop rows with missing values.\n",
    "    Note: This is a simple cleaning strategy. If you need interpolation, add it here.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required = [cfg.date_col, cfg.probe_col] + cfg.feature_cols\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing column '{c}' in {path}. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "    df[cfg.date_col] = pd.to_datetime(df[cfg.date_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[cfg.date_col]).copy()\n",
    "    df = df.sort_values(cfg.date_col).reset_index(drop=True)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df[required].copy()\n",
    "\n",
    "    # Convert feature columns to numeric\n",
    "    for c in cfg.feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with NaN in features (simple strategy)\n",
    "    df = df.dropna(subset=cfg.feature_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_by_probe(df: pd.DataFrame, cfg: Config) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Split the dataframe by probe_name, each probe becomes an independent time series.\"\"\"\n",
    "    out = {}\n",
    "    for probe_name, g in df.groupby(cfg.probe_col):\n",
    "        g = g.sort_values(cfg.date_col).reset_index(drop=True)\n",
    "        out[str(probe_name)] = g\n",
    "    return out\n",
    "\n",
    "\n",
    "def chronological_split_indices(n: int, ratios: Tuple[float, float, float]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Chronological split indices (no shuffle):\n",
    "      train: first 70%\n",
    "      val  : next 15%\n",
    "      test : last 15%\n",
    "    \"\"\"\n",
    "    r_train, r_val, r_test = ratios\n",
    "    assert abs((r_train + r_val + r_test) - 1.0) < 1e-9\n",
    "\n",
    "    n_train = int(math.floor(n * r_train))\n",
    "    n_val = int(math.floor(n * r_val))\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    idx_train = np.arange(0, n_train)\n",
    "    idx_val = np.arange(n_train, n_train + n_val)\n",
    "    idx_test = np.arange(n_train + n_val, n)\n",
    "\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Sample generation (multi-step) with context borrowing\n",
    "# ============================================================\n",
    "def make_multistep_samples(\n",
    "    X_scaled: np.ndarray,\n",
    "    y_scaled: np.ndarray,\n",
    "    dates: np.ndarray,\n",
    "    window: int,\n",
    "    horizon: int,\n",
    "    idx_split: np.ndarray,\n",
    "    idx_prev: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create multi-step supervised samples for a specific split.\n",
    "\n",
    "    Context borrowing (Scheme A):\n",
    "      - For validation: allow borrowing last `window` rows from train as input context.\n",
    "      - For test: allow borrowing last `window` rows from (train+val) as input context.\n",
    "      - Targets (y) must be strictly inside the current split => no future leakage.\n",
    "\n",
    "    Output:\n",
    "      X_seq: (num_samples, window, 1, n_features, 1)\n",
    "      y_seq: (num_samples, horizon)\n",
    "      d_seq: (num_samples,) forecast start date for each sample\n",
    "    \"\"\"\n",
    "    if len(idx_split) == 0:\n",
    "        return np.empty((0,)), np.empty((0,)), np.empty((0,))\n",
    "\n",
    "    # Build context series:\n",
    "    # If idx_prev is provided, prepend its tail (up to 'window' rows) before the split.\n",
    "    if idx_prev is None or len(idx_prev) == 0:\n",
    "        X_ctx = X_scaled[idx_split]\n",
    "        y_ctx = y_scaled[idx_split]\n",
    "        d_ctx = dates[idx_split]\n",
    "        offset = 0\n",
    "    else:\n",
    "        prev_tail = idx_prev[-window:] if len(idx_prev) >= window else idx_prev\n",
    "        ctx_idx = np.concatenate([prev_tail, idx_split])\n",
    "\n",
    "        X_ctx = X_scaled[ctx_idx]\n",
    "        y_ctx = y_scaled[ctx_idx]\n",
    "        d_ctx = dates[ctx_idx]\n",
    "        offset = len(prev_tail)\n",
    "\n",
    "    # We define a forecast start position s in the context array:\n",
    "    #   X input:  X_ctx[s-window : s]\n",
    "    #   y target: y_ctx[s : s+horizon]\n",
    "    # We require:\n",
    "    #   - s is inside the current split region => s >= offset\n",
    "    #   - full horizon target exists          => s+horizon <= len(X_ctx)\n",
    "    starts = []\n",
    "    for s in range(offset, len(X_ctx) - horizon + 1):\n",
    "        if s - window < 0:\n",
    "            continue\n",
    "        starts.append(s)\n",
    "\n",
    "    if len(starts) == 0:\n",
    "        return np.empty((0,)), np.empty((0,)), np.empty((0,))\n",
    "\n",
    "    n_features = X_ctx.shape[1]\n",
    "    X_seq = np.zeros((len(starts), window, 1, n_features, 1), dtype=np.float32)\n",
    "    y_seq = np.zeros((len(starts), horizon), dtype=np.float32)\n",
    "    d_seq = np.zeros((len(starts),), dtype=\"datetime64[ns]\")\n",
    "\n",
    "    for i, s in enumerate(starts):\n",
    "        Xw = X_ctx[s - window: s]                # shape (window, n_features)\n",
    "        yh = y_ctx[s: s + horizon].reshape(-1)   # shape (horizon,)\n",
    "\n",
    "        X_seq[i, :, 0, :, 0] = Xw\n",
    "        y_seq[i, :] = yh\n",
    "        d_seq[i] = d_ctx[s]                      # forecast start date\n",
    "\n",
    "    return X_seq, y_seq, d_seq\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model: ConvLSTM (small capacity)\n",
    "# ============================================================\n",
    "def build_convlstm_model(window: int, n_features: int, horizon: int, cfg: Config) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    ConvLSTM-style model for daily tabular time series.\n",
    "    We treat the feature dimension as a \"spatial axis\": (1 x n_features), channel=1.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inp = layers.Input(shape=(window, 1, n_features, 1))\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=cfg.filters,\n",
    "        kernel_size=cfg.kernel_size,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        return_sequences=False\n",
    "    )(inp)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(cfg.dropout)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(cfg.dropout)(x)\n",
    "\n",
    "    out = layers.Dense(horizon, activation=\"linear\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)\n",
    "    model.compile(optimizer=opt, loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plotting & metrics\n",
    "# ============================================================\n",
    "def plot_loss(history: tf.keras.callbacks.History, out_png: str, title: str):\n",
    "    \"\"\"Plot train and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n",
    "    plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss (MSE on scaled y)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_true_vs_pred(dates: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray, out_png: str, title: str, ylabel: str):\n",
    "    \"\"\"Plot actual vs predicted series.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(dates, y_true, label=\"actual\")\n",
    "    plt.plot(dates, y_pred, label=\"prediction\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "\n",
    "def evaluate_multistep(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate multi-step predictions.\n",
    "    y_true, y_pred: shape (N, H) in original scale.\n",
    "    \"\"\"\n",
    "    H = y_true.shape[1]\n",
    "    metrics = {}\n",
    "\n",
    "    metrics[\"MAE_t+1\"] = float(mean_absolute_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    metrics[\"RMSE_t+1\"] = rmse(y_true[:, 0], y_pred[:, 0])\n",
    "\n",
    "    metrics[f\"MAE_t+{H}\"] = float(mean_absolute_error(y_true[:, H - 1], y_pred[:, H - 1]))\n",
    "    metrics[f\"RMSE_t+{H}\"] = rmse(y_true[:, H - 1], y_pred[:, H - 1])\n",
    "\n",
    "    mae_steps = []\n",
    "    rmse_steps = []\n",
    "    for k in range(H):\n",
    "        mae_steps.append(mean_absolute_error(y_true[:, k], y_pred[:, k]))\n",
    "        rmse_steps.append(rmse(y_true[:, k], y_pred[:, k]))\n",
    "\n",
    "    metrics[\"MAE_avg\"] = float(np.mean(mae_steps))\n",
    "    metrics[\"RMSE_avg\"] = float(np.mean(rmse_steps))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training pipeline per (region, probe, window)\n",
    "# ============================================================\n",
    "def run_one_experiment(df_probe: pd.DataFrame, region: str, probe: str, window: int, cfg: Config):\n",
    "    \"\"\"\n",
    "    Train/evaluate/save artifacts for one probe and one window size.\n",
    "\n",
    "    Key points:\n",
    "      - Scalers are fitted on TRAIN only (no leakage).\n",
    "      - Validation borrows input context from TRAIN tail.\n",
    "      - Test borrows input context from (TRAIN+VAL) tail.\n",
    "      - Targets remain inside VAL/TEST splits.\n",
    "    \"\"\"\n",
    "    out_dir = os.path.join(cfg.output_root, f\"window_{window}\", region, probe)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare arrays\n",
    "    dates = df_probe[cfg.date_col].values\n",
    "    X = df_probe[cfg.feature_cols].values.astype(np.float32)  # (N, F)\n",
    "    y = df_probe[[cfg.target_col]].values.astype(np.float32)  # (N, 1)\n",
    "\n",
    "    N, F = X.shape\n",
    "\n",
    "    # Basic sanity check: ensure we have enough overall rows\n",
    "    if N < (window + cfg.horizon + 5):\n",
    "        print(f\"[SKIP] {region}/{probe} window={window} horizon={cfg.horizon} -> too few rows: {N}\")\n",
    "        return\n",
    "\n",
    "    # Split indices (chronological)\n",
    "    idx_train, idx_val, idx_test = chronological_split_indices(N, cfg.split_ratio)\n",
    "\n",
    "    # Fit scalers on TRAIN only\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_x.fit(X[idx_train])\n",
    "    scaler_y.fit(y[idx_train])\n",
    "\n",
    "    Xs = scaler_x.transform(X)\n",
    "    ys = scaler_y.transform(y)\n",
    "\n",
    "    # Generate samples:\n",
    "    # Train samples use only train indices (no borrowing)\n",
    "    X_train, y_train, d_train = make_multistep_samples(\n",
    "        Xs, ys, dates, window, cfg.horizon, idx_train, idx_prev=None\n",
    "    )\n",
    "\n",
    "    # Validation borrows from train tail\n",
    "    X_val, y_val, d_val = make_multistep_samples(\n",
    "        Xs, ys, dates, window, cfg.horizon, idx_val, idx_prev=idx_train\n",
    "    )\n",
    "\n",
    "    # Test borrows from (train + val) tail (recommended)\n",
    "    idx_prev_test = np.concatenate([idx_train, idx_val])\n",
    "    X_test, y_test, d_test = make_multistep_samples(\n",
    "        Xs, ys, dates, window, cfg.horizon, idx_test, idx_prev=idx_prev_test\n",
    "    )\n",
    "\n",
    "    # If any split cannot form samples, skip\n",
    "    if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
    "        print(f\"[SKIP] {region}/{probe} window={window} -> empty split samples \"\n",
    "              f\"(train={len(X_train)}, val={len(X_val)}, test={len(X_test)})\")\n",
    "        return\n",
    "\n",
    "    # Debug info (helps you understand sample counts)\n",
    "    print(f\"[INFO] {region}/{probe} window={window} | rows={N} | \"\n",
    "          f\"samples train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "\n",
    "    # Build a small ConvLSTM model\n",
    "    model = build_convlstm_model(window=window, n_features=F, horizon=cfg.horizon, cfg=cfg)\n",
    "\n",
    "    # Callbacks: early stopping prevents unnecessary training\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=cfg.max_epochs,\n",
    "        batch_size=cfg.batch_size,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save history\n",
    "    hist_df = pd.DataFrame({\n",
    "        \"epoch\": np.arange(len(history.history[\"loss\"])),\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"]\n",
    "    })\n",
    "    hist_csv = os.path.join(out_dir, \"history.csv\")\n",
    "    hist_df.to_csv(hist_csv, index=False)\n",
    "\n",
    "    # Plot loss curve\n",
    "    loss_png = os.path.join(out_dir, \"loss_train_val.png\")\n",
    "    plot_loss(history, loss_png, title=f\"LOSS: {region}/{probe} (window={window}, horizon={cfg.horizon})\")\n",
    "\n",
    "    # Predict on test\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)  # (Ntest, H)\n",
    "\n",
    "    # Inverse scaling safely: flatten -> inverse -> reshape\n",
    "    y_true = scaler_y.inverse_transform(y_test.reshape(-1, 1)).reshape(-1, cfg.horizon)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(-1, cfg.horizon)\n",
    "\n",
    "    # Align x-axis to TARGET dates (more intuitive for step plots)\n",
    "    d_test_dt = pd.to_datetime(d_test)\n",
    "\n",
    "    # Step 1 plot (t+1)\n",
    "    step1_dates = d_test_dt + pd.to_timedelta(1, unit=\"D\")\n",
    "    test_step1_png = os.path.join(out_dir, \"test_true_vs_pred_step1.png\")\n",
    "    plot_true_vs_pred(\n",
    "        dates=step1_dates,\n",
    "        y_true=y_true[:, 0],\n",
    "        y_pred=y_pred[:, 0],\n",
    "        out_png=test_step1_png,\n",
    "        title=f\"Test True vs Pred (t+1): {region}/{probe} | window={window}\",\n",
    "        ylabel=\"sm_30cm (m3/m3)\"\n",
    "    )\n",
    "\n",
    "    # Step H plot (t+H)\n",
    "    stepH_dates = d_test_dt + pd.to_timedelta(cfg.horizon, unit=\"D\")\n",
    "    test_stepH_png = os.path.join(out_dir, f\"test_true_vs_pred_step{cfg.horizon}.png\")\n",
    "    plot_true_vs_pred(\n",
    "        dates=stepH_dates,\n",
    "        y_true=y_true[:, cfg.horizon - 1],\n",
    "        y_pred=y_pred[:, cfg.horizon - 1],\n",
    "        out_png=test_stepH_png,\n",
    "        title=f\"Test True vs Pred (t+{cfg.horizon}): {region}/{probe} | window={window}\",\n",
    "        ylabel=\"sm_30cm (m3/m3)\"\n",
    "    )\n",
    "\n",
    "    # Save test comparison CSV (long format)\n",
    "    rows = []\n",
    "    for i in range(len(d_test_dt)):\n",
    "        start_date = d_test_dt[i]\n",
    "        for k in range(cfg.horizon):\n",
    "            target_date = start_date + pd.to_timedelta(k + 1, unit=\"D\")\n",
    "            rows.append({\n",
    "                \"forecast_start_date\": start_date,\n",
    "                \"target_date\": target_date,\n",
    "                \"step\": k + 1,\n",
    "                \"y_true\": float(y_true[i, k]),\n",
    "                \"y_pred\": float(y_pred[i, k]),\n",
    "            })\n",
    "    test_csv = os.path.join(out_dir, \"test_compare_multistep.csv\")\n",
    "    pd.DataFrame(rows).to_csv(test_csv, index=False)\n",
    "\n",
    "    # Compute and save summary metrics\n",
    "    metrics = evaluate_multistep(y_true, y_pred)\n",
    "    metrics.update({\n",
    "        \"region\": region,\n",
    "        \"probe_name\": probe,\n",
    "        \"window\": window,\n",
    "        \"horizon\": cfg.horizon,\n",
    "        \"n_rows\": int(N),\n",
    "        \"n_train_samples\": int(len(X_train)),\n",
    "        \"n_val_samples\": int(len(X_val)),\n",
    "        \"n_test_samples\": int(len(X_test)),\n",
    "        \"filters\": cfg.filters,\n",
    "        \"dropout\": cfg.dropout,\n",
    "    })\n",
    "    metrics_path = os.path.join(out_dir, \"summary_metrics.json\")\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Save model and scalers (for reuse)\n",
    "    model_path = os.path.join(out_dir, \"model.h5\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    joblib.dump(scaler_x, os.path.join(out_dir, \"scaler_x.pkl\"))\n",
    "    joblib.dump(scaler_y, os.path.join(out_dir, \"scaler_y.pkl\"))\n",
    "\n",
    "    # Forecast next H days from the LAST available day in this probe\n",
    "    last_X_window = Xs[-window:]  # scaled features for the last window\n",
    "    last_X_seq = last_X_window.reshape(1, window, 1, F, 1).astype(np.float32)\n",
    "\n",
    "    future_pred_scaled = model.predict(last_X_seq, verbose=0).reshape(-1, 1)  # (H,1)\n",
    "    future_pred = scaler_y.inverse_transform(future_pred_scaled).reshape(-1)  # (H,)\n",
    "\n",
    "    last_date = pd.to_datetime(df_probe[cfg.date_col].iloc[-1])\n",
    "    future_dates = [last_date + pd.to_timedelta(i + 1, unit=\"D\") for i in range(cfg.horizon)]\n",
    "\n",
    "    future_df = pd.DataFrame({\"forecast_date\": future_dates, \"pred_sm_30cm\": future_pred})\n",
    "    future_csv = os.path.join(out_dir, f\"last_date_forecast_next_{cfg.horizon}_days.csv\")\n",
    "    future_df.to_csv(future_csv, index=False)\n",
    "\n",
    "    # Forecast plot\n",
    "    future_png = os.path.join(out_dir, f\"last_date_forecast_next_{cfg.horizon}_days.png\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(future_df[\"forecast_date\"], future_df[\"pred_sm_30cm\"], marker=\"o\")\n",
    "    plt.title(f\"Forecast next {cfg.horizon} days from last date: {region}/{probe} | window={window}\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"pred sm_30cm (m3/m3)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(future_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[OK] Saved outputs -> {out_dir}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main runner\n",
    "# ============================================================\n",
    "def main():\n",
    "    cfg = Config()\n",
    "\n",
    "    # --- Update these paths if your local paths differ ---\n",
    "    cfg.csv_paths = {\n",
    "        \"Grandvillers_Sec\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers_Sec.csv\",\n",
    "        \"Grandvillers_Canon\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Canon.csv\",\n",
    "        \"Grandvillers_Robot_20\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot-20.csv\",\n",
    "        \"Grandvillers_Robot\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot.csv\",\n",
    "    }\n",
    "\n",
    "    # 8 input variables (including sm_30cm as an autoregressive feature)\n",
    "    cfg.feature_cols = [\n",
    "        \"sm_30cm\",\n",
    "        \"irrig_mm\",\n",
    "        \"IRRAD\",\n",
    "        \"TMIN\",\n",
    "        \"TMAX\",\n",
    "        \"VAP\",\n",
    "        \"WIND\",\n",
    "        \"RAIN\",\n",
    "    ]\n",
    "\n",
    "    # Window sizes to compare\n",
    "    cfg.window_list = [14, 30]  # you can set [14] only if you want\n",
    "\n",
    "    # Small model configuration\n",
    "    cfg.filters = 16   # try 8 if you want an even smaller model\n",
    "    cfg.dropout = 0.2\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "    os.makedirs(cfg.output_root, exist_ok=True)\n",
    "\n",
    "    all_probes = set()\n",
    "\n",
    "    for region, path in cfg.csv_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = load_and_clean_csv(path, cfg)\n",
    "        probe_map = split_by_probe(df, cfg)\n",
    "\n",
    "        print(f\"\\nRegion={region} | probes={list(probe_map.keys())} (count={len(probe_map)})\")\n",
    "        for p in probe_map.keys():\n",
    "            all_probes.add(p)\n",
    "\n",
    "        for probe_name, df_probe in probe_map.items():\n",
    "            for window in cfg.window_list:\n",
    "                run_one_experiment(df_probe, region, probe_name, window, cfg)\n",
    "\n",
    "    print(\"\\nAll unique probe_name values across 4 CSV:\")\n",
    "    print(sorted(list(all_probes)))\n",
    "    print(f\"Total unique probe_name: {len(all_probes)}\")\n",
    "    print(f\"\\nDone. Outputs saved under: {cfg.output_root}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ConvLSTM model successfully captured the smooth temporal dynamics of soil moisture under normal conditions. However, abrupt drops observed at the end of the test period were present in the original measurements and were not predictable from historical meteorological and soil moisture inputs. This indicates that the model learned stable hydrological patterns rather than sensor anomalies or abrupt external disturbances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndvi_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
