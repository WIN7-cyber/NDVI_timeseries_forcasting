{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Between ConvLSTM_V3 and ConvLSTM_V2\n",
    "\n",
    "Unlike ConvLSTM_V2, which adopts a direct multi-step forecasting strategy to predict multiple future time steps simultaneously, ConvLSTM_V3 reformulates the task as a single-step forecasting problem, focusing exclusively on predicting the next time step (t+1).\n",
    "\n",
    "By reducing the output layer from parallel multi-horizon outputs to a single prediction target, ConvLSTM_V3 allows the model to allocate its full representational capacity to short-term forecasting, thereby avoiding gradient interference among different forecast horizons. This simplification leads to a more stable training process and improved short-term prediction accuracy.\n",
    "\n",
    "ConvLSTM_V3 serves as a baseline model for evaluating the impact of multi-step forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM_V3 maintains the same core architecture and hyperparameter settings as ConvLSTM_V2, including the number of filters, kernel size, and input sequence length. The primary difference lies in the forecasting objective: V2 performs direct multi-step prediction over seven future time steps, whereas V3 focuses on single-step prediction (t+1), serving as a baseline for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文解释：与 ConvLSTM_V2 采用直接多步预测策略不同，ConvLSTM_V3 将预测任务简化为 单步预测问题，仅对未来一个时间步（t+1）进行预测。通过将输出层由多维并行输出（t+1 至 t+7）调整为单一输出，模型能够将全部表达能力集中于短期预测目标，从而避免多步预测中不同预测步长之间的梯度干扰。\n",
    "\n",
    "此外，单步预测设置降低了模型训练难度与不确定性，使模型在验证集上的收敛过程更加稳定。ConvLSTM_V3 的设计旨在作为多步预测模型的对照基线，用于评估多步预测策略对短期预测精度的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 类别          | 参数 / 设置          | **V2（多步预测）**  | **V3（单步预测）** | 说明     |\n",
    "| ----------- | ---------------- | ------------- | ------------ | ------ |\n",
    "| **预测任务**    | forecasting type | multi-step    | single-step  | 任务定义不同 |\n",
    "| **预测步长**    | `horizon`        | **7**         | **1**        | 最核心差异  |\n",
    "| **输出层**     | `Dense(...)`     | `Dense(7)`    | `Dense(1)`   | 输出维度不同 |\n",
    "| **预测目标**    | y                | `[t+1 … t+7]` | `t+1`        | 标签构造不同 |\n",
    "| **loss 计算** | loss             | 多步平均          | 单步 loss      | 优化目标不同 |\n",
    "| **标签维度**    | `y.shape`        | `(N, 7)`      | `(N, 1)`     | 数据结构不同 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那 train 里：\n",
    "\n",
    "第一个训练样本：\n",
    "\n",
    "X：第 1–14 天\n",
    "\n",
    "y：第 15 天\n",
    "\n",
    "第二个训练样本：\n",
    "\n",
    "X：第 2–15 天\n",
    "\n",
    "y：第 16 天\n",
    "\n",
    "…\n",
    "\n",
    "最后一个训练样本：\n",
    "\n",
    "X：第 56–69 天\n",
    "\n",
    "y：第 70 天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # CSV paths (update if needed)\n",
    "    csv_paths: Dict[str, str] = None\n",
    "\n",
    "    # Columns\n",
    "    date_col: str = \"date\"\n",
    "    probe_col: str = \"probe_name\"\n",
    "\n",
    "    # Feature set (8 variables)\n",
    "    feature_cols: List[str] = None\n",
    "    target_col: str = \"sm_30cm\"\n",
    "\n",
    "    # Time series settings\n",
    "    window_list: List[int] = None          # lookback windows to try\n",
    "    horizon: int = 1                       # ONLY t+1 for formal experiment\n",
    "    split_ratio: Tuple[float, float, float] = (0.70, 0.15, 0.15)\n",
    "\n",
    "    # Model hyperparameters (small model as requested)\n",
    "    filters: int = 16                      # you can set 8 or 16\n",
    "    dropout: float = 0.2                   # Dropout(0.2)\n",
    "    kernel_size: Tuple[int, int] = (1, 3)  # convolution over feature axis (cols)\n",
    "    batch_size: int = 16\n",
    "    max_epochs: int = 150\n",
    "    learning_rate: float = 1e-3\n",
    "\n",
    "    # Output root folder\n",
    "    output_root: str = \"outputs_tplus1_smallwin\"\n",
    "\n",
    "    # Reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Fix random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data utilities\n",
    "# -----------------------------\n",
    "def load_and_clean_csv(path: str, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV, parse date, keep required columns, and drop missing values.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required = [cfg.date_col, cfg.probe_col] + cfg.feature_cols\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing column '{c}' in {path}. Available: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "    df[cfg.date_col] = pd.to_datetime(df[cfg.date_col])\n",
    "    df = df.sort_values(cfg.date_col).reset_index(drop=True)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df[required].copy()\n",
    "\n",
    "    # Convert numeric columns\n",
    "    for c in cfg.feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with missing essential values\n",
    "    df = df.dropna(subset=cfg.feature_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_by_probe(df: pd.DataFrame, cfg: Config) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a dataframe into multiple dataframes keyed by probe_name.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for probe_name, g in df.groupby(cfg.probe_col):\n",
    "        g = g.sort_values(cfg.date_col).reset_index(drop=True)\n",
    "        out[str(probe_name)] = g\n",
    "    return out\n",
    "\n",
    "\n",
    "def chronological_split_indices(n: int, ratios: Tuple[float, float, float]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Chronological split indices: train -> val -> test.\n",
    "    \"\"\"\n",
    "    r_train, r_val, r_test = ratios\n",
    "    if abs((r_train + r_val + r_test) - 1.0) > 1e-9:\n",
    "        raise ValueError(\"split_ratio must sum to 1.0\")\n",
    "\n",
    "    n_train = int(math.floor(n * r_train))\n",
    "    n_val = int(math.floor(n * r_val))\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    idx_train = np.arange(0, n_train)\n",
    "    idx_val = np.arange(n_train, n_train + n_val)\n",
    "    idx_test = np.arange(n_train + n_val, n)\n",
    "\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def make_tplus1_samples_with_context_borrow(\n",
    "    X_scaled: np.ndarray,       # [N, F]\n",
    "    y_scaled: np.ndarray,       # [N, 1]\n",
    "    dates: np.ndarray,          # [N]\n",
    "    window: int,\n",
    "    idx_split: np.ndarray,      # indices for current split\n",
    "    idx_prev: Optional[np.ndarray] = None  # previous split indices (train for val, val for test)\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create t+1 samples for one split, using Scheme A (context borrowing):\n",
    "    - For val: prepend last `window` rows from train as input context\n",
    "    - For test: prepend last `window` rows from val as input context\n",
    "    - Targets (y) are strictly inside the current split (no leakage)\n",
    "\n",
    "    For t+1:\n",
    "    - Input window: X[t-window : t]\n",
    "    - Target: y[t]  (interpreted as \"next day\" if you define t as next day index)\n",
    "    Here we align the label to be the day right after the window ends.\n",
    "\n",
    "    Returns:\n",
    "      X_seq: [num_samples, window, 1, F, 1]\n",
    "      y_seq: [num_samples, 1]   (still keep 2D for Keras)\n",
    "      d_seq: [num_samples]      forecast start date (the day we are predicting)\n",
    "    \"\"\"\n",
    "    if len(idx_split) == 0:\n",
    "        return np.empty((0,)), np.empty((0,)), np.empty((0,))\n",
    "\n",
    "    # Build context arrays\n",
    "    if idx_prev is None or len(idx_prev) == 0:\n",
    "        ctx_idx = idx_split\n",
    "        offset = 0\n",
    "    else:\n",
    "        prev_tail = idx_prev[-window:] if len(idx_prev) >= window else idx_prev\n",
    "        ctx_idx = np.concatenate([prev_tail, idx_split])\n",
    "        offset = len(prev_tail)\n",
    "\n",
    "    X_ctx = X_scaled[ctx_idx]\n",
    "    y_ctx = y_scaled[ctx_idx]\n",
    "    d_ctx = dates[ctx_idx]\n",
    "\n",
    "    starts = []\n",
    "    # We want the target index s to be inside the \"current split part\" => s >= offset\n",
    "    # Need enough history: s-window >= 0\n",
    "    for s in range(offset, len(X_ctx)):\n",
    "        if s - window < 0:\n",
    "            continue\n",
    "        # t+1 label is y at s (the day being predicted)\n",
    "        starts.append(s)\n",
    "\n",
    "    if len(starts) == 0:\n",
    "        return np.empty((0,)), np.empty((0,)), np.empty((0,))\n",
    "\n",
    "    F = X_ctx.shape[1]\n",
    "    X_seq = np.zeros((len(starts), window, 1, F, 1), dtype=np.float32)\n",
    "    y_seq = np.zeros((len(starts), 1), dtype=np.float32)\n",
    "    d_seq = np.zeros((len(starts),), dtype=\"datetime64[ns]\")\n",
    "\n",
    "    for i, s in enumerate(starts):\n",
    "        Xw = X_ctx[s - window: s]     # [window, F]\n",
    "        yt = y_ctx[s]                 # [1]\n",
    "        X_seq[i, :, 0, :, 0] = Xw\n",
    "        y_seq[i, 0] = float(yt)\n",
    "        d_seq[i] = d_ctx[s]           # date of the target day\n",
    "\n",
    "    return X_seq, y_seq, d_seq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "def build_convlstm_model(window: int, n_features: int, cfg: Config) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    ConvLSTM-style model for tabular daily time series.\n",
    "    We treat features as a \"spatial axis\":\n",
    "      input shape = (time, rows=1, cols=n_features, channels=1)\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inp = layers.Input(shape=(window, 1, n_features, 1))\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=cfg.filters,\n",
    "        kernel_size=cfg.kernel_size,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        return_sequences=False\n",
    "    )(inp)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(cfg.dropout)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(cfg.dropout)(x)\n",
    "\n",
    "    # horizon = 1 => output one value (t+1)\n",
    "    out = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)\n",
    "    model.compile(optimizer=opt, loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting / metrics\n",
    "# -----------------------------\n",
    "def plot_loss(history: tf.keras.callbacks.History, out_png: str, title: str):\n",
    "    \"\"\"Plot training and validation loss.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n",
    "    plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss (MSE on scaled y)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_true_vs_pred(dates: np.ndarray, y_true: np.ndarray, y_pred: np.ndarray, out_png: str, title: str, ylabel: str):\n",
    "    \"\"\"Plot true vs predicted series.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(dates, y_true, label=\"actual\")\n",
    "    plt.plot(dates, y_pred, label=\"prediction\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# One experiment: (region, probe, window)\n",
    "# -----------------------------\n",
    "def run_one_experiment(df_probe: pd.DataFrame, region: str, probe: str, window: int, cfg: Config):\n",
    "    \"\"\"\n",
    "    Train / evaluate / save outputs for one probe and one window size (t+1 only).\n",
    "    \"\"\"\n",
    "    out_dir = os.path.join(cfg.output_root, f\"window_{window}\", region, probe)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare arrays\n",
    "    dates = df_probe[cfg.date_col].values\n",
    "    X = df_probe[cfg.feature_cols].values.astype(np.float32)     # [N, F]\n",
    "    y = df_probe[[cfg.target_col]].values.astype(np.float32)     # [N, 1]\n",
    "\n",
    "    N, F = X.shape\n",
    "    # Minimal sanity: need enough points for window and some val/test\n",
    "    if N < (window + 10):\n",
    "        print(f\"[SKIP] {region}/{probe} window={window} -> too few rows: {N}\")\n",
    "        return\n",
    "\n",
    "    # Chronological split indices\n",
    "    idx_train, idx_val, idx_test = chronological_split_indices(N, cfg.split_ratio)\n",
    "\n",
    "    # Fit scalers on TRAIN ONLY (no leakage)\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_x.fit(X[idx_train])\n",
    "    scaler_y.fit(y[idx_train])\n",
    "\n",
    "    Xs = scaler_x.transform(X)\n",
    "    ys = scaler_y.transform(y)\n",
    "\n",
    "    # Build samples with Scheme A context borrowing\n",
    "    X_train, y_train, d_train = make_tplus1_samples_with_context_borrow(\n",
    "        Xs, ys, dates, window, idx_train, idx_prev=None\n",
    "    )\n",
    "    X_val, y_val, d_val = make_tplus1_samples_with_context_borrow(\n",
    "        Xs, ys, dates, window, idx_val, idx_prev=idx_train\n",
    "    )\n",
    "    X_test, y_test, d_test = make_tplus1_samples_with_context_borrow(\n",
    "        Xs, ys, dates, window, idx_test, idx_prev=idx_val\n",
    "    )\n",
    "\n",
    "    if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
    "        print(f\"[SKIP] {region}/{probe} window={window} -> empty samples \"\n",
    "              f\"(train={len(X_train)}, val={len(X_val)}, test={len(X_test)})\")\n",
    "        return\n",
    "\n",
    "    # Build model\n",
    "    model = build_convlstm_model(window=window, n_features=F, cfg=cfg)\n",
    "\n",
    "    # Callbacks: early stop often stops before max_epochs (this is normal)\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=cfg.max_epochs,\n",
    "        batch_size=cfg.batch_size,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save training history\n",
    "    hist_df = pd.DataFrame({\n",
    "        \"epoch\": np.arange(len(history.history[\"loss\"])),\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"]\n",
    "    })\n",
    "    hist_df.to_csv(os.path.join(out_dir, \"history.csv\"), index=False)\n",
    "\n",
    "    # Plot loss curve\n",
    "    plot_loss(\n",
    "        history,\n",
    "        out_png=os.path.join(out_dir, \"loss_train_val.png\"),\n",
    "        title=f\"LOSS: {region}/{probe} (window={window}, horizon=1)\"\n",
    "    )\n",
    "\n",
    "    # Predict on test\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)   # [Ntest, 1]\n",
    "\n",
    "    # Inverse scale back to original units\n",
    "    y_true = scaler_y.inverse_transform(y_test)        # [Ntest, 1]\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled) # [Ntest, 1]\n",
    "\n",
    "    # Compute metrics (t+1)\n",
    "    mae = float(mean_absolute_error(y_true[:, 0], y_pred[:, 0]))\n",
    "    r = rmse(y_true[:, 0], y_pred[:, 0])\n",
    "\n",
    "    metrics = {\n",
    "        \"region\": region,\n",
    "        \"probe_name\": probe,\n",
    "        \"window\": window,\n",
    "        \"horizon\": 1,\n",
    "        \"n_rows\": int(N),\n",
    "        \"n_train_samples\": int(len(X_train)),\n",
    "        \"n_val_samples\": int(len(X_val)),\n",
    "        \"n_test_samples\": int(len(X_test)),\n",
    "        \"filters\": cfg.filters,\n",
    "        \"dropout\": cfg.dropout,\n",
    "        \"MAE_t+1\": mae,\n",
    "        \"RMSE_t+1\": r\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"summary_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Plot test true vs pred (t+1)\n",
    "    test_dates = pd.to_datetime(d_test)  # d_test already corresponds to target date\n",
    "    plot_true_vs_pred(\n",
    "        dates=test_dates,\n",
    "        y_true=y_true[:, 0],\n",
    "        y_pred=y_pred[:, 0],\n",
    "        out_png=os.path.join(out_dir, \"test_true_vs_pred_tplus1.png\"),\n",
    "        title=f\"Test True vs Pred (t+1): {region}/{probe} | window={window}\",\n",
    "        ylabel=\"sm_30cm (m3/m3)\"\n",
    "    )\n",
    "\n",
    "    # Save test compare CSV\n",
    "    test_df = pd.DataFrame({\n",
    "        \"target_date\": test_dates,\n",
    "        \"y_true\": y_true[:, 0],\n",
    "        \"y_pred\": y_pred[:, 0]\n",
    "    })\n",
    "    test_df.to_csv(os.path.join(out_dir, \"test_compare_tplus1.csv\"), index=False)\n",
    "\n",
    "    # Save model and scalers\n",
    "    model.save(os.path.join(out_dir, \"model.h5\"))\n",
    "    joblib.dump(scaler_x, os.path.join(out_dir, \"scaler_x.pkl\"))\n",
    "    joblib.dump(scaler_y, os.path.join(out_dir, \"scaler_y.pkl\"))\n",
    "\n",
    "    # Forecast the next day AFTER the last available date\n",
    "    # Use last `window` X values as input\n",
    "    last_X_window = Xs[-window:]                                  # [window, F] scaled\n",
    "    last_X_seq = last_X_window.reshape(1, window, 1, F, 1)        # [1, window, 1, F, 1]\n",
    "    next_day_scaled = model.predict(last_X_seq, verbose=0)        # [1, 1]\n",
    "    next_day_value = scaler_y.inverse_transform(next_day_scaled)[0, 0]\n",
    "\n",
    "    last_date = pd.to_datetime(df_probe[cfg.date_col].iloc[-1])\n",
    "    next_date = last_date + pd.to_timedelta(1, unit=\"D\")\n",
    "\n",
    "    future_df = pd.DataFrame({\n",
    "        \"forecast_date\": [next_date],\n",
    "        \"pred_sm_30cm\": [float(next_day_value)]\n",
    "    })\n",
    "    future_df.to_csv(os.path.join(out_dir, \"last_date_forecast_next_1_day.csv\"), index=False)\n",
    "\n",
    "    print(f\"[OK] {region}/{probe} window={window} -> saved to: {out_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region=Grandvillers_Sec | probes=['Sec'] (count=1)\n",
      "Epoch 1/150\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 1.0917 - val_loss: 0.9375 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8878 - val_loss: 0.9547 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6054 - val_loss: 0.9400 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5214 - val_loss: 0.9352 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3993 - val_loss: 0.9327 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4681 - val_loss: 0.9476 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3930 - val_loss: 0.9578 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3051 - val_loss: 0.9669 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4871 - val_loss: 0.9723 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.2210\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3871 - val_loss: 0.9713 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3600 - val_loss: 0.9719 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4047 - val_loss: 1.0091 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3535 - val_loss: 1.0573 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2838 - val_loss: 1.1007 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.3181\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3723 - val_loss: 1.1046 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Sec/Sec window=14 -> saved to: outputs_tplus1_smallwin\\window_14\\Grandvillers_Sec\\Sec\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 2.2762 - val_loss: 0.6250 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1857 - val_loss: 0.6691 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.4482 - val_loss: 0.6746 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7972 - val_loss: 0.6651 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7560 - val_loss: 0.6690 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5245\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6362 - val_loss: 0.6701 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7041 - val_loss: 0.6731 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4700 - val_loss: 0.6795 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4997 - val_loss: 0.6880 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4728 - val_loss: 0.6949 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5077\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5806 - val_loss: 0.7020 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Sec/Sec window=30 -> saved to: outputs_tplus1_smallwin\\window_30\\Grandvillers_Sec\\Sec\n",
      "\n",
      "Region=Grandvillers_Canon | probes=['Canon1', 'Canon2'] (count=2)\n",
      "Epoch 1/150\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0835 - val_loss: 0.5281 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.6013 - val_loss: 0.5771 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.5034 - val_loss: 0.5837 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.4140 - val_loss: 0.5575 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1267 - val_loss: 0.5198 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8915 - val_loss: 0.4939 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.0109 - val_loss: 0.4750 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7237 - val_loss: 0.4579 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4846 - val_loss: 0.4370 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7828 - val_loss: 0.4168 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6183 - val_loss: 0.3906 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6049 - val_loss: 0.3744 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4393 - val_loss: 0.3492 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7107 - val_loss: 0.3315 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9021 - val_loss: 0.3373 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6101 - val_loss: 0.3556 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4213 - val_loss: 0.3752 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7927 - val_loss: 0.4094 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4710\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5110 - val_loss: 0.4309 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6355 - val_loss: 0.4283 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3822 - val_loss: 0.4223 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6234 - val_loss: 0.4220 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5129 - val_loss: 0.4302 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5159\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4479 - val_loss: 0.4379 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Canon/Canon1 window=14 -> saved to: outputs_tplus1_smallwin\\window_14\\Grandvillers_Canon\\Canon1\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 13.8758 - val_loss: 0.2736 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 4.1745 - val_loss: 0.3356 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9053 - val_loss: 0.3785 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1990 - val_loss: 0.3950 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1211 - val_loss: 0.4002 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.2480\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.4522 - val_loss: 0.3889 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.2192 - val_loss: 0.3821 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.8397 - val_loss: 0.3687 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0381 - val_loss: 0.3551 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5496 - val_loss: 0.3446 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.8503\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6698 - val_loss: 0.3394 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Canon/Canon1 window=30 -> saved to: outputs_tplus1_smallwin\\window_30\\Grandvillers_Canon\\Canon1\n",
      "Epoch 1/150\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.1894 - val_loss: 0.4150 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.2119 - val_loss: 0.3935 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7744 - val_loss: 0.4071 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8756 - val_loss: 0.4079 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6133 - val_loss: 0.4011 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7632 - val_loss: 0.3975 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5344 - val_loss: 0.3875 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7247 - val_loss: 0.3756 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4691 - val_loss: 0.3690 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3777 - val_loss: 0.3533 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4481 - val_loss: 0.3423 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4711 - val_loss: 0.3391 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4988 - val_loss: 0.3530 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7078 - val_loss: 0.3737 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6477 - val_loss: 0.3870 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4110 - val_loss: 0.3977 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.2962\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3669 - val_loss: 0.3975 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4941 - val_loss: 0.3920 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3772 - val_loss: 0.3852 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4479 - val_loss: 0.3851 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3685 - val_loss: 0.3819 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.2739\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2468 - val_loss: 0.3811 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Canon/Canon2 window=14 -> saved to: outputs_tplus1_smallwin\\window_14\\Grandvillers_Canon\\Canon2\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 3.4310 - val_loss: 0.9704 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.0530 - val_loss: 0.9178 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.5279 - val_loss: 0.8737 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.1754 - val_loss: 0.8358 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.0768 - val_loss: 0.8282 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.8508 - val_loss: 0.8195 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.8347 - val_loss: 0.8417 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7381 - val_loss: 0.8698 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6898 - val_loss: 0.8724 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6746 - val_loss: 0.8574 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6637 - val_loss: 0.8179 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7400 - val_loss: 0.7810 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5314 - val_loss: 0.7676 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5793 - val_loss: 0.7647 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4408 - val_loss: 0.7764 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4055 - val_loss: 0.7933 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7019 - val_loss: 0.7986 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5033 - val_loss: 0.8051 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7175 - val_loss: 0.8112 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6467 - val_loss: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6624 - val_loss: 0.8134 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4068 - val_loss: 0.8213 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5182 - val_loss: 0.8335 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3090\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4419 - val_loss: 0.8461 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Canon/Canon2 window=30 -> saved to: outputs_tplus1_smallwin\\window_30\\Grandvillers_Canon\\Canon2\n",
      "\n",
      "Region=Grandvillers_Robot_20 | probes=['Robot-20'] (count=1)\n",
      "Epoch 1/150\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.1749 - val_loss: 1.2311 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8802 - val_loss: 1.2368 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1020 - val_loss: 1.2187 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7658 - val_loss: 1.2057 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6914 - val_loss: 1.1812 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6150 - val_loss: 1.1581 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6717 - val_loss: 1.1373 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6395 - val_loss: 1.1132 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5868 - val_loss: 1.0880 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4832 - val_loss: 1.0649 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5294 - val_loss: 1.0387 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4553 - val_loss: 1.0267 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3697 - val_loss: 1.0281 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3675 - val_loss: 1.0292 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2882 - val_loss: 1.0306 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4454 - val_loss: 1.0470 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.2575\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 1.0535 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3061 - val_loss: 1.0493 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3113 - val_loss: 1.0472 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4102 - val_loss: 1.0543 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3614 - val_loss: 1.0583 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1514\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2892 - val_loss: 1.0532 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Robot_20/Robot-20 window=14 -> saved to: outputs_tplus1_smallwin\\window_14\\Grandvillers_Robot_20\\Robot-20\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 3.0953 - val_loss: 1.6923 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.9835 - val_loss: 1.6817 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5684 - val_loss: 1.6945 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5756 - val_loss: 1.7361 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4907 - val_loss: 1.7945 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6956 - val_loss: 1.8259 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7409\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5460 - val_loss: 1.8392 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4846 - val_loss: 1.8436 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6601 - val_loss: 1.8315 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3533 - val_loss: 1.8248 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7947 - val_loss: 1.8254 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3732\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3844 - val_loss: 1.8501 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Robot_20/Robot-20 window=30 -> saved to: outputs_tplus1_smallwin\\window_30\\Grandvillers_Robot_20\\Robot-20\n",
      "\n",
      "Region=Grandvillers_Robot | probes=['Canon3', 'Robot'] (count=2)\n",
      "Epoch 1/150\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.5813 - val_loss: 0.5308 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1946 - val_loss: 0.4707 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.4276 - val_loss: 0.3914 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0778 - val_loss: 0.3770 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0546 - val_loss: 0.4154 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7336 - val_loss: 0.4671 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1189 - val_loss: 0.4701 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5588 - val_loss: 0.4731 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9599\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6341 - val_loss: 0.4745 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5537 - val_loss: 0.4795 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3369 - val_loss: 0.4851 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6644 - val_loss: 0.4919 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5339 - val_loss: 0.4970 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9322\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7904 - val_loss: 0.4965 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Robot/Canon3 window=14 -> saved to: outputs_tplus1_smallwin\\window_14\\Grandvillers_Robot\\Canon3\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 4.1888 - val_loss: 0.5493 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.2102 - val_loss: 0.6010 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.7806 - val_loss: 0.6199 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.0305 - val_loss: 0.6155 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.2024 - val_loss: 0.6210 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8463\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.8463 - val_loss: 0.6353 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.4941 - val_loss: 0.6468 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4704 - val_loss: 0.6586 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.8174 - val_loss: 0.6673 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.7059 - val_loss: 0.6742 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6177\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5985 - val_loss: 0.6729 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Robot/Canon3 window=30 -> saved to: outputs_tplus1_smallwin\\window_30\\Grandvillers_Robot\\Canon3\n",
      "Epoch 1/150\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 2.5509 - val_loss: 0.3138 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.2365 - val_loss: 0.3333 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7204 - val_loss: 0.3185 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.9861 - val_loss: 0.2998 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6881 - val_loss: 0.2856 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5321 - val_loss: 0.2643 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.4049 - val_loss: 0.2442 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4270 - val_loss: 0.2274 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5416 - val_loss: 0.2185 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4538 - val_loss: 0.2265 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3854 - val_loss: 0.2252 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.5552 - val_loss: 0.2247 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3759 - val_loss: 0.2217 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4157\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.4157 - val_loss: 0.2334 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3645 - val_loss: 0.2346 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3283 - val_loss: 0.2415 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3351 - val_loss: 0.2503 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2989 - val_loss: 0.2500 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3826 - val_loss: 0.2466 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Robot/Robot window=14 -> saved to: outputs_tplus1_smallwin\\window_14\\Grandvillers_Robot\\Robot\n",
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.8343 - val_loss: 0.3277 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.1744 - val_loss: 0.3238 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.8669 - val_loss: 0.3146 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.7599 - val_loss: 0.3030 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4351 - val_loss: 0.3010 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6431 - val_loss: 0.3004 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.7084 - val_loss: 0.2995 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5565 - val_loss: 0.2989 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.5293 - val_loss: 0.2925 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4553 - val_loss: 0.2923 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4963 - val_loss: 0.2978 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.3997 - val_loss: 0.3011 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5174 - val_loss: 0.3018 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3630 - val_loss: 0.3015 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2830\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3029 - val_loss: 0.3004 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.5636 - val_loss: 0.3008 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3522 - val_loss: 0.3014 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3746 - val_loss: 0.3009 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2836 - val_loss: 0.3004 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.3348\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.3348 - val_loss: 0.2993 - lr: 5.0000e-04\n",
      "[OK] Grandvillers_Robot/Robot window=30 -> saved to: outputs_tplus1_smallwin\\window_30\\Grandvillers_Robot\\Robot\n",
      "\n",
      "All unique probe_name values across 4 CSV:\n",
      "['Canon1', 'Canon2', 'Canon3', 'Robot', 'Robot-20', 'Sec']\n",
      "Total unique probe_name: 6\n",
      "\n",
      "Done. Outputs saved under: outputs_tplus1_smallwin\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    cfg = Config()\n",
    "\n",
    "    # Your provided paths\n",
    "    cfg.csv_paths = {\n",
    "        \"Grandvillers_Sec\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers_Sec.csv\",\n",
    "        \"Grandvillers_Canon\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Canon.csv\",\n",
    "        \"Grandvillers_Robot_20\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot-20.csv\",\n",
    "        \"Grandvillers_Robot\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot.csv\",\n",
    "    }\n",
    "\n",
    "    # Your 8 input variables (keep sm_30cm in X as autoregressive feature)\n",
    "    cfg.feature_cols = [\n",
    "        \"sm_30cm\",\n",
    "        \"irrig_mm\",\n",
    "        \"IRRAD\",\n",
    "        \"TMIN\",\n",
    "        \"TMAX\",\n",
    "        \"VAP\",\n",
    "        \"WIND\",\n",
    "        \"RAIN\",\n",
    "    ]\n",
    "\n",
    "    # Only t+1 (formal)\n",
    "    cfg.horizon = 1\n",
    "\n",
    "    # Windows you want to test\n",
    "    cfg.window_list = [14, 30]  # you can change to [14] only\n",
    "\n",
    "    # Small model settings (as you requested)\n",
    "    cfg.filters = 16     # or 8\n",
    "    cfg.dropout = 0.2\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "    os.makedirs(cfg.output_root, exist_ok=True)\n",
    "\n",
    "    all_probes = set()\n",
    "\n",
    "    for region, path in cfg.csv_paths.items():\n",
    "        df = load_and_clean_csv(path, cfg)\n",
    "        probes = split_by_probe(df, cfg)\n",
    "\n",
    "        print(f\"\\nRegion={region} | probes={list(probes.keys())} (count={len(probes)})\")\n",
    "        for p in probes.keys():\n",
    "            all_probes.add(p)\n",
    "\n",
    "        for probe_name, df_probe in probes.items():\n",
    "            for window in cfg.window_list:\n",
    "                run_one_experiment(df_probe, region, probe_name, window, cfg)\n",
    "\n",
    "    print(\"\\nAll unique probe_name values across 4 CSV:\")\n",
    "    print(sorted(list(all_probes)))\n",
    "    print(f\"Total unique probe_name: {len(all_probes)}\")\n",
    "    print(f\"\\nDone. Outputs saved under: {cfg.output_root}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndvi_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
