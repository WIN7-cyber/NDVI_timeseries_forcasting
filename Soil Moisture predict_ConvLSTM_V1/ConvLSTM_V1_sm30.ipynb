{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI 仓库：输入是 时空栅格序列（H×W×channels），ConvLSTM 非常自然。\n",
    "\n",
    "现在：是 单站点 8 个变量的时间序列，没有空间维度。为了仍然用 ConvLSTM，我采用了“把 8 个变量铺成 1×8 的网格”，让 ConvLSTM2D 可以吃进去，这属于把 NDVI 思路迁移到你的数据形态上。\n",
    "\n",
    "如果想更“原汁原味”的 ConvLSTM（像 NDVI 那样有空间维），需要有空间网格数据；现在的数据更像“多变量时间序列”，LSTM/GRU 其实更合适，但要求 ConvLSTM，我们就用这个迁移做法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM\n",
    "用 8 个变量铺成 1×8×1 的“伪空间网格”\n",
    "\n",
    "输入 shape：(batch, time, 1, 8, 1)\n",
    "\n",
    "输出：(batch, 1) 的 sm_30cm(t+1)\n",
    "\n",
    "\n",
    "按时间顺序切分 train/val/test = 70%/15%/15%\n",
    "\n",
    "用 train 训练，用 val 早停/调学习率\n",
    "\n",
    "用 test 做指标评估（MAE / RMSE）\n",
    "\n",
    "同时输出：每个区域最后日期的下一天（t+1）sm_30cm 预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必须保存：\n",
    "1️⃣ 模型（ConvLSTM 权重 + 结构）\n",
    "2️⃣ X 的 scaler\n",
    "3️⃣ y 的 scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "# Input features (8 variables) and target (one-step ahead)\n",
    "FEATURE_COLS = [\"sm_30cm\", \"irrig_mm\", \"IRRAD\", \"TMIN\", \"TMAX\", \"VAP\", \"WIND\", \"RAIN\"]\n",
    "TARGET_COL = \"sm_30cm\"\n",
    "\n",
    "# Column names in your CSV\n",
    "DATE_COL = \"date\"\n",
    "PROBE_COL = \"probe_name\"   # <-- confirmed by you\n",
    "\n",
    "# Time-series supervised learning settings\n",
    "LOOKBACK = 14              # use past 14 days to predict next day\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15           # remaining 0.15 is test\n",
    "\n",
    "# Training settings\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Make a string safe for filesystem paths.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", s)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_csv(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV and keep only required columns.\n",
    "    This matches the 'data preparation' step used in the reference idea:\n",
    "    clean -> sort by time -> build sequences.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Basic schema checks\n",
    "    for col in [DATE_COL, PROBE_COL]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"[{csv_path}] Missing required column: {col}\")\n",
    "\n",
    "    missing = [c for c in FEATURE_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"[{csv_path}] Missing feature columns: {missing}\")\n",
    "\n",
    "    # Parse dates and numeric columns\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[DATE_COL]).copy()\n",
    "\n",
    "    for c in FEATURE_COLS:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    df = df[[DATE_COL, PROBE_COL] + FEATURE_COLS].copy()\n",
    "\n",
    "    # Sort to preserve time order within each probe (critical for time-series split)\n",
    "    df = df.sort_values([PROBE_COL, DATE_COL]).reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_one_probe(df_probe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean a single probe time series:\n",
    "    - sort by date\n",
    "    - time interpolation for missing values\n",
    "    - forward/backward fill as fallback\n",
    "    \"\"\"\n",
    "    df_probe = df_probe.sort_values(DATE_COL).copy()\n",
    "    df_probe = df_probe.set_index(DATE_COL)\n",
    "\n",
    "    df_probe[FEATURE_COLS] = df_probe[FEATURE_COLS].interpolate(\n",
    "        method=\"time\", limit_direction=\"both\"\n",
    "    )\n",
    "    df_probe[FEATURE_COLS] = df_probe[FEATURE_COLS].ffill().bfill()\n",
    "\n",
    "    df_probe = df_probe.reset_index()\n",
    "    df_probe = df_probe.dropna(subset=FEATURE_COLS).reset_index(drop=True)\n",
    "    return df_probe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_time(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Time-ordered split (no shuffling) into train/val/test = 70/15/15.\n",
    "    This avoids leakage and matches standard forecasting practice.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val = int(n * VAL_RATIO)\n",
    "\n",
    "    train = df.iloc[:n_train].copy()\n",
    "    val = df.iloc[n_train:n_train + n_val].copy()\n",
    "    test = df.iloc[n_train + n_val:].copy()\n",
    "\n",
    "    # Each split must be long enough to build sequences\n",
    "    for name, part in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "        if len(part) < LOOKBACK + 2:\n",
    "            raise ValueError(\n",
    "                f\"Split '{name}' too short: {len(part)} rows. \"\n",
    "                f\"Need at least LOOKBACK+2={LOOKBACK+2}. \"\n",
    "                f\"Reduce LOOKBACK or use longer series.\"\n",
    "            )\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised(\n",
    "    df: pd.DataFrame,\n",
    "    scaler_x: StandardScaler,\n",
    "    scaler_y: StandardScaler,\n",
    "    fit: bool\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build supervised samples for one-step-ahead forecasting:\n",
    "    X(t-LOOKBACK ... t-1) -> y(t)\n",
    "\n",
    "    IMPORTANT: To reuse ConvLSTM from the reference repo's idea,\n",
    "    we reshape the 8 tabular variables into a pseudo 2D grid (rows=1, cols=8),\n",
    "    so ConvLSTM2D can process sequences of \"frames\" with shape (1, 8, 1).\n",
    "\n",
    "    Output X shape:\n",
    "      (samples, time=LOOKBACK, rows=1, cols=8, channels=1)\n",
    "    \"\"\"\n",
    "    X_raw = df[FEATURE_COLS].values.astype(np.float32)\n",
    "    y_raw = df[[TARGET_COL]].values.astype(np.float32)\n",
    "    dates = df[DATE_COL].values  # aligned with y\n",
    "\n",
    "    if fit:\n",
    "        scaler_x.fit(X_raw)\n",
    "        scaler_y.fit(y_raw)\n",
    "\n",
    "    Xs = scaler_x.transform(X_raw)\n",
    "    ys = scaler_y.transform(y_raw)\n",
    "\n",
    "    X_list, y_list, d_list = [], [], []\n",
    "    for i in range(LOOKBACK, len(df)):\n",
    "        X_list.append(Xs[i - LOOKBACK:i, :])  # (LOOKBACK, 8)\n",
    "        y_list.append(ys[i, 0])               # y at time i (scaled)\n",
    "        d_list.append(dates[i])               # date of y\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)              # (N, LOOKBACK, 8)\n",
    "    y = np.array(y_list, dtype=np.float32).reshape(-1, 1)\n",
    "    d = np.array(d_list)\n",
    "\n",
    "    # Reshape tabular features into pseudo image for ConvLSTM2D\n",
    "    X = X.reshape(X.shape[0], LOOKBACK, 1, len(FEATURE_COLS), 1)\n",
    "    return X, y, d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_convlstm(lookback: int, width: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    ConvLSTM backbone (inspired by the reference repo's ConvLSTM forecasting idea).\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(lookback, 1, width, 1))\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=16,\n",
    "        kernel_size=(1, 3),\n",
    "        padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"tanh\",\n",
    "    )(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=16,\n",
    "        kernel_size=(1, 3),\n",
    "        padding=\"same\",\n",
    "        return_sequences=False,\n",
    "        activation=\"tanh\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def inverse_y(scaler_y: StandardScaler, y_scaled_2d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Inverse transform scaled y back to original units.\"\"\"\n",
    "    return scaler_y.inverse_transform(y_scaled_2d).reshape(-1)\n",
    "\n",
    "\n",
    "def plot_loss(history: tf.keras.callbacks.History, out_png: str, title: str) -> None:\n",
    "    \"\"\"Plot train/validation loss curves (single figure).\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss (MSE on scaled y)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_test_true_vs_pred(dates, y_true, y_pred, out_png: str, title: str) -> None:\n",
    "    \"\"\"Plot test series: actual vs prediction (like the example screenshot).\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(dates, y_true, label=\"actual\")\n",
    "    plt.plot(dates, y_pred, label=\"prediction\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"sm_30cm (m3/m3)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training / Evaluation per probe\n",
    "# ============================================================\n",
    "def train_eval_predict_one_probe(region: str, probe: str, df_probe: pd.DataFrame, out_root: str = \"outputs\") -> None:\n",
    "    \"\"\"\n",
    "    For each probe (sub-region):\n",
    "    - Clean data\n",
    "    - Time split 70/15/15\n",
    "    - Train ConvLSTM\n",
    "    - Save model + scalers + history CSV\n",
    "    - Plot loss curve and test true-vs-pred curve\n",
    "    - Report test metrics and next-day prediction\n",
    "    \"\"\"\n",
    "    df_probe = clean_one_probe(df_probe)\n",
    "\n",
    "    # minimal sanity check\n",
    "    if len(df_probe) < LOOKBACK + 10:\n",
    "        print(f\"[SKIP] {region}/{probe} too short: {len(df_probe)} rows\")\n",
    "        return\n",
    "\n",
    "    train_df, val_df, test_df = split_by_time(df_probe)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train, y_train, _ = make_supervised(train_df, scaler_x, scaler_y, fit=True)\n",
    "    X_val, y_val, _ = make_supervised(val_df, scaler_x, scaler_y, fit=False)\n",
    "    X_test, y_test, d_test = make_supervised(test_df, scaler_x, scaler_y, fit=False)\n",
    "\n",
    "    model = build_convlstm(LOOKBACK, len(FEATURE_COLS))\n",
    "\n",
    "    # Output directory per (region, probe)\n",
    "    probe_dir = os.path.join(out_root, safe_name(region), safe_name(probe))\n",
    "    os.makedirs(probe_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(probe_dir, \"model.h5\")\n",
    "    scaler_x_path = os.path.join(probe_dir, \"scaler_x.pkl\")\n",
    "    scaler_y_path = os.path.join(probe_dir, \"scaler_y.pkl\")\n",
    "    history_csv = os.path.join(probe_dir, \"history.csv\")\n",
    "    loss_png = os.path.join(probe_dir, \"loss_train_val.png\")\n",
    "    test_png = os.path.join(probe_dir, \"test_true_vs_pred.png\")\n",
    "    test_csv = os.path.join(probe_dir, \"test_compare.csv\")\n",
    "\n",
    "    # Callbacks:\n",
    "    # - ModelCheckpoint: keep best weights based on val_loss\n",
    "    # - EarlyStopping: stop when no improvement\n",
    "    # - ReduceLROnPlateau: stabilize optimization\n",
    "    # - CSVLogger: store per-epoch loss/val_loss\n",
    "    cbs = [\n",
    "        callbacks.ModelCheckpoint(model_path, monitor=\"val_loss\", save_best_only=True),\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-5),\n",
    "        callbacks.CSVLogger(history_csv, append=False),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n=== TRAINING: {region} / {probe} ===\")\n",
    "    print(f\"Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}  (lookback={LOOKBACK})\")\n",
    "\n",
    "    # verbose=1 prints train/val loss each epoch, as you requested\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=cbs,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save scalers to guarantee consistent normalization at inference time\n",
    "    joblib.dump(scaler_x, scaler_x_path)\n",
    "    joblib.dump(scaler_y, scaler_y_path)\n",
    "\n",
    "    # Plot train/val loss (single figure)\n",
    "    plot_loss(history, loss_png, title=f\"Loss: {region}/{probe}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Test evaluation (inverse transform to original units)\n",
    "    # ------------------------------------------------------------\n",
    "    y_pred_test_scaled = model.predict(X_test, verbose=0)     # scaled predictions\n",
    "    y_true = inverse_y(scaler_y, y_test)                      # original units\n",
    "    y_pred = inverse_y(scaler_y, y_pred_test_scaled)          # original units\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Percentage error (MAPE) to quantify \"prediction vs true in percent\"\n",
    "    denom = np.maximum(np.abs(y_true), 1e-8)\n",
    "    mape = float(np.mean(np.abs((y_pred - y_true) / denom)) * 100.0)\n",
    "\n",
    "    print(f\"[TEST] MAE={mae:.6f} RMSE={rmse:.6f} MAPE={mape:.2f}%  (unit: m3/m3)\")\n",
    "\n",
    "    # Save test comparison table (date, true, pred, percent error)\n",
    "    pct_err = (y_pred - y_true) / denom * 100.0\n",
    "    cmp_df = pd.DataFrame({\n",
    "        \"date\": pd.to_datetime(d_test),\n",
    "        \"y_true_sm_30cm\": y_true,\n",
    "        \"y_pred_sm_30cm\": y_pred,\n",
    "        \"pct_error_%\": pct_err\n",
    "    })\n",
    "    cmp_df.to_csv(test_csv, index=False)\n",
    "\n",
    "    # Plot test curve (actual vs prediction)\n",
    "    plot_test_true_vs_pred(pd.to_datetime(d_test), y_true, y_pred, test_png,\n",
    "                           title=f\"Test True vs Pred: {region}/{probe}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Next-day prediction: last available date + 1 day\n",
    "    # ------------------------------------------------------------\n",
    "    last_dt = pd.to_datetime(df_probe[DATE_COL].iloc[-1])\n",
    "    next_dt = last_dt + pd.Timedelta(days=1)\n",
    "\n",
    "    last_window = df_probe[FEATURE_COLS].iloc[-LOOKBACK:].values.astype(np.float32)\n",
    "    X_last = scaler_x.transform(last_window).reshape(1, LOOKBACK, 1, len(FEATURE_COLS), 1)\n",
    "\n",
    "    y_next_scaled = model.predict(X_last, verbose=0)\n",
    "    y_next = scaler_y.inverse_transform(y_next_scaled)[0, 0]\n",
    "\n",
    "    print(f\"[NEXT DAY] last_date={last_dt.date()} -> predict_date={next_dt.date()}  \"\n",
    "          f\"pred_sm_30cm={y_next:.6f} (m3/m3)\")\n",
    "    print(f\"Saved outputs to: {probe_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region=Grandvillers_Sec  probes=['Sec']  (count=1)\n",
      "\n",
      "=== TRAINING: Grandvillers_Sec / Sec ===\n",
      "Samples: train=66, val=3, test=4  (lookback=14)\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 2.0923 - val_loss: 0.3485 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.8616 - val_loss: 0.3540 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6797 - val_loss: 0.3602 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4922 - val_loss: 0.3627 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3383 - val_loss: 0.3696 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4168 - val_loss: 0.3745 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2963 - val_loss: 0.3744 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2596 - val_loss: 0.3670 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2174 - val_loss: 0.3609 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2988 - val_loss: 0.3563 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1948 - val_loss: 0.3533 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2788 - val_loss: 0.3503 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1902 - val_loss: 0.3483 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.3110 - val_loss: 0.3465 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.2311 - val_loss: 0.3420 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1587 - val_loss: 0.3364 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1357 - val_loss: 0.3299 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1748 - val_loss: 0.3229 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2120 - val_loss: 0.3140 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1839 - val_loss: 0.3064 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1461 - val_loss: 0.2988 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1532 - val_loss: 0.2943 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1993 - val_loss: 0.2902 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1661 - val_loss: 0.2843 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1398 - val_loss: 0.2789 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1314 - val_loss: 0.2727 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1768 - val_loss: 0.2683 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1236 - val_loss: 0.2656 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1545 - val_loss: 0.2595 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1254 - val_loss: 0.2528 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0971 - val_loss: 0.2481 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1037 - val_loss: 0.2412 - lr: 5.0000e-04\n",
      "Epoch 33/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1581 - val_loss: 0.2325 - lr: 5.0000e-04\n",
      "Epoch 34/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1185 - val_loss: 0.2242 - lr: 5.0000e-04\n",
      "Epoch 35/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1145 - val_loss: 0.2174 - lr: 5.0000e-04\n",
      "Epoch 36/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1160 - val_loss: 0.2105 - lr: 5.0000e-04\n",
      "Epoch 37/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1024 - val_loss: 0.2069 - lr: 5.0000e-04\n",
      "Epoch 38/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0866 - val_loss: 0.2042 - lr: 5.0000e-04\n",
      "Epoch 39/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1374 - val_loss: 0.2010 - lr: 5.0000e-04\n",
      "Epoch 40/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1106 - val_loss: 0.1943 - lr: 5.0000e-04\n",
      "Epoch 41/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1010 - val_loss: 0.1865 - lr: 5.0000e-04\n",
      "Epoch 42/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1022 - val_loss: 0.1769 - lr: 5.0000e-04\n",
      "Epoch 43/150\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0840 - val_loss: 0.1707 - lr: 5.0000e-04\n",
      "Epoch 44/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0723 - val_loss: 0.1678 - lr: 5.0000e-04\n",
      "Epoch 45/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0983 - val_loss: 0.1631 - lr: 5.0000e-04\n",
      "Epoch 46/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1068 - val_loss: 0.1570 - lr: 5.0000e-04\n",
      "Epoch 47/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0583 - val_loss: 0.1481 - lr: 5.0000e-04\n",
      "Epoch 48/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0941 - val_loss: 0.1399 - lr: 5.0000e-04\n",
      "Epoch 49/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0814 - val_loss: 0.1323 - lr: 5.0000e-04\n",
      "Epoch 50/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0808 - val_loss: 0.1270 - lr: 5.0000e-04\n",
      "Epoch 51/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0769 - val_loss: 0.1215 - lr: 5.0000e-04\n",
      "Epoch 52/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0852 - val_loss: 0.1170 - lr: 5.0000e-04\n",
      "Epoch 53/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0897 - val_loss: 0.1143 - lr: 5.0000e-04\n",
      "Epoch 54/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0731 - val_loss: 0.1142 - lr: 5.0000e-04\n",
      "Epoch 55/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0568 - val_loss: 0.1141 - lr: 5.0000e-04\n",
      "Epoch 56/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0840 - val_loss: 0.1117 - lr: 5.0000e-04\n",
      "Epoch 57/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0924 - val_loss: 0.1064 - lr: 5.0000e-04\n",
      "Epoch 58/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0741 - val_loss: 0.0989 - lr: 5.0000e-04\n",
      "Epoch 59/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0877 - val_loss: 0.0908 - lr: 5.0000e-04\n",
      "Epoch 60/150\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0725 - val_loss: 0.0833 - lr: 5.0000e-04\n",
      "Epoch 61/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0833 - val_loss: 0.0763 - lr: 5.0000e-04\n",
      "Epoch 62/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0917 - val_loss: 0.0690 - lr: 5.0000e-04\n",
      "Epoch 63/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0727 - val_loss: 0.0611 - lr: 5.0000e-04\n",
      "Epoch 64/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0816 - val_loss: 0.0544 - lr: 5.0000e-04\n",
      "Epoch 65/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0629 - val_loss: 0.0479 - lr: 5.0000e-04\n",
      "Epoch 66/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0595 - val_loss: 0.0422 - lr: 5.0000e-04\n",
      "Epoch 67/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0615 - val_loss: 0.0364 - lr: 5.0000e-04\n",
      "Epoch 68/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0830 - val_loss: 0.0324 - lr: 5.0000e-04\n",
      "Epoch 69/150\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0666 - val_loss: 0.0304 - lr: 5.0000e-04\n",
      "Epoch 70/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0675 - val_loss: 0.0276 - lr: 5.0000e-04\n",
      "Epoch 71/150\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0677 - val_loss: 0.0255 - lr: 5.0000e-04\n",
      "Epoch 72/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0665 - val_loss: 0.0237 - lr: 5.0000e-04\n",
      "Epoch 73/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0639 - val_loss: 0.0216 - lr: 5.0000e-04\n",
      "Epoch 74/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0599 - val_loss: 0.0193 - lr: 5.0000e-04\n",
      "Epoch 75/150\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0566 - val_loss: 0.0179 - lr: 5.0000e-04\n",
      "Epoch 76/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0774 - val_loss: 0.0190 - lr: 5.0000e-04\n",
      "Epoch 77/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0676 - val_loss: 0.0226 - lr: 5.0000e-04\n",
      "Epoch 78/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0657 - val_loss: 0.0241 - lr: 5.0000e-04\n",
      "Epoch 79/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0648 - val_loss: 0.0199 - lr: 5.0000e-04\n",
      "Epoch 80/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0641 - val_loss: 0.0140 - lr: 5.0000e-04\n",
      "Epoch 81/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0748 - val_loss: 0.0085 - lr: 5.0000e-04\n",
      "Epoch 82/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0664 - val_loss: 0.0050 - lr: 5.0000e-04\n",
      "Epoch 83/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0582 - val_loss: 0.0027 - lr: 5.0000e-04\n",
      "Epoch 84/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0759 - val_loss: 0.0017 - lr: 5.0000e-04\n",
      "Epoch 85/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0476 - val_loss: 9.6311e-04 - lr: 5.0000e-04\n",
      "Epoch 86/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0621 - val_loss: 6.4004e-04 - lr: 5.0000e-04\n",
      "Epoch 87/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0409 - val_loss: 5.5536e-04 - lr: 5.0000e-04\n",
      "Epoch 88/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0561 - val_loss: 5.6316e-04 - lr: 5.0000e-04\n",
      "Epoch 89/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0702 - val_loss: 5.9988e-04 - lr: 5.0000e-04\n",
      "Epoch 90/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0512 - val_loss: 6.2062e-04 - lr: 5.0000e-04\n",
      "Epoch 91/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0671 - val_loss: 7.7611e-04 - lr: 5.0000e-04\n",
      "Epoch 92/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0591 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 93/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0566 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 94/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0434 - val_loss: 0.0032 - lr: 2.5000e-04\n",
      "Epoch 95/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0442 - val_loss: 0.0050 - lr: 2.5000e-04\n",
      "Epoch 96/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0468 - val_loss: 0.0068 - lr: 2.5000e-04\n",
      "Epoch 97/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0461 - val_loss: 0.0090 - lr: 2.5000e-04\n",
      "Epoch 98/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0500 - val_loss: 0.0117 - lr: 2.5000e-04\n",
      "Epoch 99/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0342 - val_loss: 0.0141 - lr: 2.5000e-04\n",
      "Epoch 100/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0440 - val_loss: 0.0170 - lr: 2.5000e-04\n",
      "Epoch 101/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0580 - val_loss: 0.0190 - lr: 1.2500e-04\n",
      "Epoch 102/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0403 - val_loss: 0.0216 - lr: 1.2500e-04\n",
      "[TEST] MAE=0.476921 RMSE=0.480714 MAPE=1.83%  (unit: m3/m3)\n",
      "[NEXT DAY] last_date=2025-08-27 -> predict_date=2025-08-28  pred_sm_30cm=26.511492 (m3/m3)\n",
      "Saved outputs to: outputs\\Grandvillers_Sec\\Sec\n",
      "\n",
      "Region=Grandvillers_Canon  probes=['Canon1', 'Canon2']  (count=2)\n",
      "\n",
      "=== TRAINING: Grandvillers_Canon / Canon1 ===\n",
      "Samples: train=67, val=3, test=4  (lookback=14)\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 1.5892 - val_loss: 0.2401 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.8446 - val_loss: 0.2533 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.8688 - val_loss: 0.2582 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4206 - val_loss: 0.2546 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4607 - val_loss: 0.2444 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5587 - val_loss: 0.2368 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.3703 - val_loss: 0.2353 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3435 - val_loss: 0.2339 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3078 - val_loss: 0.2355 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2858 - val_loss: 0.2360 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2655 - val_loss: 0.2314 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3108 - val_loss: 0.2241 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2566 - val_loss: 0.2166 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2908 - val_loss: 0.2080 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2060 - val_loss: 0.1989 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.2886 - val_loss: 0.1899 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1675 - val_loss: 0.1814 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2994 - val_loss: 0.1722 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1909 - val_loss: 0.1650 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1834 - val_loss: 0.1615 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1771 - val_loss: 0.1613 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1768 - val_loss: 0.1694 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2029 - val_loss: 0.1843 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2006 - val_loss: 0.1872 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1562 - val_loss: 0.1854 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1155 - val_loss: 0.1829 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1112 - val_loss: 0.1826 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1454 - val_loss: 0.1795 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1141 - val_loss: 0.1772 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1066 - val_loss: 0.1734 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1001 - val_loss: 0.1705 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1094 - val_loss: 0.1683 - lr: 5.0000e-04\n",
      "Epoch 33/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1120 - val_loss: 0.1689 - lr: 5.0000e-04\n",
      "Epoch 34/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1567 - val_loss: 0.1703 - lr: 5.0000e-04\n",
      "Epoch 35/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1375 - val_loss: 0.1712 - lr: 5.0000e-04\n",
      "Epoch 36/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0749 - val_loss: 0.1704 - lr: 2.5000e-04\n",
      "[TEST] MAE=5.017222 RMSE=7.396986 MAPE=35.68%  (unit: m3/m3)\n",
      "[NEXT DAY] last_date=2025-08-28 -> predict_date=2025-08-29  pred_sm_30cm=25.788010 (m3/m3)\n",
      "Saved outputs to: outputs\\Grandvillers_Canon\\Canon1\n",
      "\n",
      "=== TRAINING: Grandvillers_Canon / Canon2 ===\n",
      "Samples: train=66, val=3, test=4  (lookback=14)\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 2.0502 - val_loss: 0.0174 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.7380 - val_loss: 0.0134 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4799 - val_loss: 0.0120 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5339 - val_loss: 0.0107 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5410 - val_loss: 0.0096 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5471 - val_loss: 0.0091 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.3294 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4477 - val_loss: 0.0082 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3962 - val_loss: 0.0077 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.3342 - val_loss: 0.0072 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3409 - val_loss: 0.0069 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3117 - val_loss: 0.0071 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2734 - val_loss: 0.0073 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4112 - val_loss: 0.0072 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2607 - val_loss: 0.0078 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3575 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4035 - val_loss: 0.0099 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3172 - val_loss: 0.0113 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3882 - val_loss: 0.0121 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3058 - val_loss: 0.0128 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2221 - val_loss: 0.0134 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1920 - val_loss: 0.0142 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2098 - val_loss: 0.0151 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1786 - val_loss: 0.0157 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2760 - val_loss: 0.0160 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2209 - val_loss: 0.0159 - lr: 2.5000e-04\n",
      "[TEST] MAE=0.424130 RMSE=0.430737 MAPE=1.63%  (unit: m3/m3)\n",
      "[NEXT DAY] last_date=2025-08-27 -> predict_date=2025-08-28  pred_sm_30cm=26.541903 (m3/m3)\n",
      "Saved outputs to: outputs\\Grandvillers_Canon\\Canon2\n",
      "\n",
      "Region=Grandvillers_Robot_20  probes=['Robot-20']  (count=1)\n",
      "\n",
      "=== TRAINING: Grandvillers_Robot_20 / Robot-20 ===\n",
      "Samples: train=65, val=2, test=4  (lookback=14)\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 1.4449 - val_loss: 0.9274 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.8347 - val_loss: 0.9129 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.7354 - val_loss: 0.8970 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6108 - val_loss: 0.9000 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5803 - val_loss: 0.9124 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4761 - val_loss: 0.9206 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4440 - val_loss: 0.9167 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4743 - val_loss: 0.9005 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5127 - val_loss: 0.8835 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5052 - val_loss: 0.8704 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3852 - val_loss: 0.8614 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5324 - val_loss: 0.8477 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3877 - val_loss: 0.8292 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3364 - val_loss: 0.8231 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3738 - val_loss: 0.8143 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3445 - val_loss: 0.8120 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5545 - val_loss: 0.8087 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2779 - val_loss: 0.8093 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5040 - val_loss: 0.8207 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4450 - val_loss: 0.8353 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3847 - val_loss: 0.8460 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4335 - val_loss: 0.8625 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3999 - val_loss: 0.8798 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4260 - val_loss: 0.8805 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2270 - val_loss: 0.8817 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2342 - val_loss: 0.8774 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2340 - val_loss: 0.8729 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2157 - val_loss: 0.8711 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2607 - val_loss: 0.8661 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2637 - val_loss: 0.8533 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3271 - val_loss: 0.8468 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3239 - val_loss: 0.8523 - lr: 2.5000e-04\n",
      "[TEST] MAE=0.731867 RMSE=0.734027 MAPE=2.62%  (unit: m3/m3)\n",
      "[NEXT DAY] last_date=2025-08-27 -> predict_date=2025-08-28  pred_sm_30cm=27.319038 (m3/m3)\n",
      "Saved outputs to: outputs\\Grandvillers_Robot_20\\Robot-20\n",
      "\n",
      "Region=Grandvillers_Robot  probes=['Canon3', 'Robot']  (count=2)\n",
      "\n",
      "=== TRAINING: Grandvillers_Robot / Canon3 ===\n",
      "Samples: train=66, val=3, test=4  (lookback=14)\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 2.6249 - val_loss: 0.2711 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 1.1137 - val_loss: 0.2566 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5471 - val_loss: 0.2282 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.5141 - val_loss: 0.2082 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6036 - val_loss: 0.2021 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4283 - val_loss: 0.2055 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2800 - val_loss: 0.2094 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3695 - val_loss: 0.2102 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3965 - val_loss: 0.2099 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.5034 - val_loss: 0.2105 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2917 - val_loss: 0.2123 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3086 - val_loss: 0.2138 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3544 - val_loss: 0.2145 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3433 - val_loss: 0.2162 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2767 - val_loss: 0.2174 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2208 - val_loss: 0.2179 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2835 - val_loss: 0.2171 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1893 - val_loss: 0.2147 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2964 - val_loss: 0.2106 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2021 - val_loss: 0.2092 - lr: 2.5000e-04\n",
      "[TEST] MAE=0.137201 RMSE=0.145456 MAPE=0.50%  (unit: m3/m3)\n",
      "[NEXT DAY] last_date=2025-08-27 -> predict_date=2025-08-28  pred_sm_30cm=27.472279 (m3/m3)\n",
      "Saved outputs to: outputs\\Grandvillers_Robot\\Canon3\n",
      "\n",
      "=== TRAINING: Grandvillers_Robot / Robot ===\n",
      "Samples: train=66, val=3, test=4  (lookback=14)\n",
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 1.6067 - val_loss: 0.0647 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.8067 - val_loss: 0.0591 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4906 - val_loss: 0.0639 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6199 - val_loss: 0.0693 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4928 - val_loss: 0.0722 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3155 - val_loss: 0.0696 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4283 - val_loss: 0.0681 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4645 - val_loss: 0.0679 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3719 - val_loss: 0.0685 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3966 - val_loss: 0.0693 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.5475 - val_loss: 0.0711 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2810 - val_loss: 0.0726 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3315 - val_loss: 0.0735 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3061 - val_loss: 0.0723 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3166 - val_loss: 0.0722 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2409 - val_loss: 0.0727 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3229 - val_loss: 0.0722 - lr: 2.5000e-04\n",
      "[TEST] MAE=0.384254 RMSE=0.396399 MAPE=1.53%  (unit: m3/m3)\n",
      "[NEXT DAY] last_date=2025-08-27 -> predict_date=2025-08-28  pred_sm_30cm=25.466103 (m3/m3)\n",
      "Saved outputs to: outputs\\Grandvillers_Robot\\Robot\n",
      "\n",
      "All unique probe_name values across 4 CSV:\n",
      "['Canon1', 'Canon2', 'Canon3', 'Robot', 'Robot-20', 'Sec']\n",
      "Total unique probe_name: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Main: 4 CSV files -> split by probe_name -> train/eval each\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    REGION_CSVS: Dict[str, str] = {\n",
    "        \"Grandvillers_Sec\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers_Sec.csv\",\n",
    "        \"Grandvillers_Canon\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Canon.csv\",\n",
    "        \"Grandvillers_Robot_20\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot-20.csv\",\n",
    "        \"Grandvillers_Robot\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot.csv\",\n",
    "    }\n",
    "\n",
    "    all_probes = set()\n",
    "\n",
    "    for region, path in REGION_CSVS.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = load_csv(path)\n",
    "\n",
    "        probes = sorted(df[PROBE_COL].dropna().unique().tolist())\n",
    "        all_probes.update(probes)\n",
    "\n",
    "        print(f\"\\nRegion={region}  probes={probes}  (count={len(probes)})\")\n",
    "\n",
    "        for probe in probes:\n",
    "            df_probe = df[df[PROBE_COL] == probe].copy()\n",
    "            train_eval_predict_one_probe(region, probe, df_probe, out_root=\"outputs\")\n",
    "\n",
    "    print(\"\\nAll unique probe_name values across 4 CSV:\")\n",
    "    print(sorted(list(all_probes)))\n",
    "    print(f\"Total unique probe_name: {len(all_probes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  在短期、深层土壤水分预测中，ConvLSTM 很容易退化为均值预测器 \n",
    "输入是 (time, features)，没有空间维度\n",
    "\n",
    "ConvLSTM 在这里 理论上不合适\n",
    "\n",
    "原仓库用 ConvLSTM 是因为 NDVI 是栅格（H×W）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进： \n",
    "1. 增大时间窗口：让模型看到“慢变量的累积效应”，而不是只看到短期噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 预测多步（t+H）不是为了“看得更远”，\n",
    "而是为了“逼模型必须学动态”，而不能靠惯性（persistence）混过去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rows ≥ lookback + horizon + 1\n",
    "要生成 1 条训练样本，你至少需要：\n",
    "\n",
    "lookback 天作为输入\n",
    "\n",
    "horizon 天作为预测目标\n",
    "\n",
    "还要多 1 天，才能让时间索引对齐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那要怎么才能既保持 70/15/15，又跑 window=14/30/60 + multi-step？\n",
    "\n",
    "\n",
    "✅ val/test “借用”前一段的 lookback 历史（不泄漏）\n",
    "\n",
    "意思是：\n",
    "\n",
    "val 的输入序列可以包含 train 最后 lookback 天（只当历史上下文）\n",
    "\n",
    "但 val 的预测目标 y 仍然只在 val 区间\n",
    "\n",
    "test 同理借 val（或 train+val）末尾 lookback 天\n",
    "\n",
    "这样 val 不需要 ≥ lookback+horizon+1，只要 val ≥ horizon 就能跑。\n",
    "\n",
    "这就是context stitching 修改方案。\n",
    "\n",
    "这在时间序列里非常常见，因为预测时你本来也会用“过去的历史”作为上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "FEATURE_COLS = [\"sm_30cm\", \"irrig_mm\", \"IRRAD\", \"TMIN\", \"TMAX\", \"VAP\", \"WIND\", \"RAIN\"]\n",
    "TARGET_COL = \"sm_30cm\"\n",
    "\n",
    "DATE_COL = \"date\"\n",
    "PROBE_COL = \"probe_name\"   # confirmed\n",
    "\n",
    "# Forecast horizon (multi-step): predict next H days\n",
    "HORIZON = 7\n",
    "\n",
    "# Compare different lookback windows\n",
    "WINDOWS = [14, 30, 60]\n",
    "\n",
    "# Time split ratios (must remain unchanged as requested)\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15  # remaining 0.15 is test\n",
    "\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Utility functions\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Make a string safe for filesystem paths.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_csv(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV and keep required columns with basic type cleaning.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    for col in [DATE_COL, PROBE_COL]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"[{csv_path}] Missing required column: {col}\")\n",
    "\n",
    "    missing = [c for c in FEATURE_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"[{csv_path}] Missing feature columns: {missing}\")\n",
    "\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[DATE_COL]).copy()\n",
    "\n",
    "    for c in FEATURE_COLS:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = df[[DATE_COL, PROBE_COL] + FEATURE_COLS].copy()\n",
    "    df = df.sort_values([PROBE_COL, DATE_COL]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_one_probe(df_probe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean a single probe series:\n",
    "    - sort by date\n",
    "    - interpolate by time\n",
    "    - forward/backward fill\n",
    "    \"\"\"\n",
    "    df_probe = df_probe.sort_values(DATE_COL).copy()\n",
    "    df_probe = df_probe.set_index(DATE_COL)\n",
    "\n",
    "    df_probe[FEATURE_COLS] = df_probe[FEATURE_COLS].interpolate(method=\"time\", limit_direction=\"both\")\n",
    "    df_probe[FEATURE_COLS] = df_probe[FEATURE_COLS].ffill().bfill()\n",
    "\n",
    "    df_probe = df_probe.reset_index()\n",
    "    df_probe = df_probe.dropna(subset=FEATURE_COLS).reset_index(drop=True)\n",
    "    return df_probe\n",
    "\n",
    "\n",
    "def split_by_time(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Time-ordered split into train/val/test = 70/15/15 (no shuffling).\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val = int(n * VAL_RATIO)\n",
    "\n",
    "    train = df.iloc[:n_train].copy()\n",
    "    val = df.iloc[n_train:n_train + n_val].copy()\n",
    "    test = df.iloc[n_train + n_val:].copy()\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def require_train_min_length(train_df: pd.DataFrame, lookback: int, horizon: int):\n",
    "    \"\"\"\n",
    "    For training, we cannot borrow context from the past outside train split.\n",
    "    Therefore train must be long enough to build multi-step samples internally.\n",
    "    \"\"\"\n",
    "    min_len = lookback + horizon + 1\n",
    "    if len(train_df) < min_len:\n",
    "        raise ValueError(\n",
    "            f\"Train split too short: {len(train_df)} rows. Need >= {min_len} \"\n",
    "            f\"(lookback={lookback}, horizon={horizon}).\"\n",
    "        )\n",
    "\n",
    "\n",
    "def require_val_test_min_length(split_df: pd.DataFrame, horizon: int, split_name: str):\n",
    "    \"\"\"\n",
    "    With context stitching, val/test only need to be long enough to provide targets.\n",
    "    Minimum requirement: at least 'horizon' rows to form one target vector.\n",
    "    \"\"\"\n",
    "    if len(split_df) < horizon:\n",
    "        raise ValueError(\n",
    "            f\"{split_name} split too short: {len(split_df)} rows. Need >= {horizon} (horizon={horizon}).\"\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_multistep(\n",
    "    df_all: pd.DataFrame,\n",
    "    lookback: int,\n",
    "    horizon: int,\n",
    "    scaler_x: StandardScaler,\n",
    "    scaler_y: StandardScaler,\n",
    "    fit: bool\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build multi-step supervised samples from a single DataFrame (used for train).\n",
    "      X: past lookback days of 8 features\n",
    "      y: next horizon days of sm_30cm\n",
    "\n",
    "    Output:\n",
    "      X shape: (N, lookback, 1, 8, 1)\n",
    "      y shape: (N, horizon)\n",
    "      d shape: forecast start date for each sample\n",
    "    \"\"\"\n",
    "    X_raw = df_all[FEATURE_COLS].values.astype(np.float32)\n",
    "    y_raw = df_all[[TARGET_COL]].values.astype(np.float32)\n",
    "    dates = df_all[DATE_COL].values\n",
    "\n",
    "    if fit:\n",
    "        scaler_x.fit(X_raw)\n",
    "        scaler_y.fit(y_raw)\n",
    "\n",
    "    Xs = scaler_x.transform(X_raw)\n",
    "    ys = scaler_y.transform(y_raw).reshape(-1)\n",
    "\n",
    "    X_list, y_list, d_list = [], [], []\n",
    "    for i in range(lookback, len(df_all) - horizon + 1):\n",
    "        X_list.append(Xs[i - lookback:i, :])\n",
    "        y_list.append(ys[i:i + horizon])\n",
    "        d_list.append(dates[i])\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.float32)\n",
    "    d = np.array(d_list)\n",
    "\n",
    "    X = X.reshape(X.shape[0], lookback, 1, len(FEATURE_COLS), 1)\n",
    "    return X, y, d\n",
    "\n",
    "\n",
    "def make_supervised_multistep_with_context(\n",
    "    df_context: pd.DataFrame,\n",
    "    df_target: pd.DataFrame,\n",
    "    lookback: int,\n",
    "    horizon: int,\n",
    "    scaler_x: StandardScaler,\n",
    "    scaler_y: StandardScaler\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Context stitching (Solution A):\n",
    "    - We allow using the last 'lookback' days from the previous split as input context\n",
    "    - Targets (y) are strictly within df_target (no future leakage)\n",
    "\n",
    "    df_context: typically last lookback rows from the previous split\n",
    "    df_target : the split we are evaluating (val or test)\n",
    "\n",
    "    Requirement for df_target:\n",
    "      len(df_target) >= horizon  (enough target days)\n",
    "    \"\"\"\n",
    "    # Concatenate context + target, preserving time order\n",
    "    df_all = pd.concat([df_context, df_target], axis=0).copy()\n",
    "    df_all = df_all.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    X_raw = df_all[FEATURE_COLS].values.astype(np.float32)\n",
    "    y_raw = df_all[[TARGET_COL]].values.astype(np.float32)\n",
    "    dates = df_all[DATE_COL].values\n",
    "\n",
    "    Xs = scaler_x.transform(X_raw)\n",
    "    ys = scaler_y.transform(y_raw).reshape(-1)\n",
    "\n",
    "    ctx_len = len(df_context)\n",
    "\n",
    "    X_list, y_list, d_list = [], [], []\n",
    "    # i must be within the target part and y(i:i+horizon) must be fully in target\n",
    "    for i in range(max(lookback, ctx_len), len(df_all) - horizon + 1):\n",
    "        if i < ctx_len:\n",
    "            continue\n",
    "        if i + horizon - 1 < ctx_len:\n",
    "            continue\n",
    "\n",
    "        X_list.append(Xs[i - lookback:i, :])\n",
    "        y_list.append(ys[i:i + horizon])\n",
    "        d_list.append(dates[i])\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.float32)\n",
    "    d = np.array(d_list)\n",
    "\n",
    "    X = X.reshape(X.shape[0], lookback, 1, len(FEATURE_COLS), 1)\n",
    "    return X, y, d\n",
    "\n",
    "\n",
    "def build_convlstm_multistep(lookback: int, width: int, horizon: int) -> tf.keras.Model:\n",
    "    \"\"\"ConvLSTM model producing multi-step outputs (Dense(horizon)).\"\"\"\n",
    "    inp = layers.Input(shape=(lookback, 1, width, 1))\n",
    "\n",
    "    x = layers.ConvLSTM2D(filters=16, kernel_size=(1, 3),\n",
    "                          padding=\"same\", return_sequences=True, activation=\"tanh\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.ConvLSTM2D(filters=16, kernel_size=(1, 3),\n",
    "                          padding=\"same\", return_sequences=False, activation=\"tanh\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    out = layers.Dense(horizon, activation=\"linear\")(x)\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def inverse_y_vector(scaler_y: StandardScaler, y_scaled: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Inverse-transform a 2D array (N, horizon) back to original units.\"\"\"\n",
    "    flat = y_scaled.reshape(-1, 1)\n",
    "    inv = scaler_y.inverse_transform(flat).reshape(y_scaled.shape)\n",
    "    return inv\n",
    "\n",
    "\n",
    "def plot_loss(history: tf.keras.callbacks.History, out_png: str, title: str):\n",
    "    \"\"\"Plot train/val loss curves.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss (MSE on scaled y)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_series(dates, y_true, y_pred, out_png: str, title: str, ylabel: str):\n",
    "    \"\"\"Plot actual vs prediction curves.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(dates, y_true, label=\"actual\")\n",
    "    plt.plot(dates, y_pred, label=\"prediction\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def metrics_1d(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute MAE, RMSE, and MAPE (%) on 1D arrays.\"\"\"\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    denom = np.maximum(np.abs(y_true), 1e-8)\n",
    "    mape = float(np.mean(np.abs((y_pred - y_true) / denom)) * 100.0)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE_%\": mape}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Experiment runner for one (region, probe, window)\n",
    "# ============================================================\n",
    "def run_one_setting(region: str, probe: str, df_probe: pd.DataFrame, lookback: int, horizon: int, out_root: str):\n",
    "    \"\"\"\n",
    "    Train/evaluate one model for a specific lookback window:\n",
    "    - Train samples are built strictly from train split\n",
    "    - Val/test samples are built with context stitching (Solution A)\n",
    "      * val uses last lookback rows from train as context\n",
    "      * test uses last lookback rows from (train+val) as context\n",
    "    \"\"\"\n",
    "    df_probe = clean_one_probe(df_probe)\n",
    "    train_df, val_df, test_df = split_by_time(df_probe)\n",
    "\n",
    "    # Minimal length checks\n",
    "    require_train_min_length(train_df, lookback, horizon)\n",
    "    require_val_test_min_length(val_df, horizon, \"val\")\n",
    "    require_val_test_min_length(test_df, horizon, \"test\")\n",
    "\n",
    "    # Fit scalers ONLY on train split (standard practice)\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train, y_train, _ = make_supervised_multistep(\n",
    "        df_all=train_df, lookback=lookback, horizon=horizon,\n",
    "        scaler_x=scaler_x, scaler_y=scaler_y, fit=True\n",
    "    )\n",
    "\n",
    "    # Build val with context borrowed from end of train\n",
    "    val_context = train_df.iloc[-lookback:].copy()\n",
    "    X_val, y_val, _ = make_supervised_multistep_with_context(\n",
    "        df_context=val_context, df_target=val_df,\n",
    "        lookback=lookback, horizon=horizon,\n",
    "        scaler_x=scaler_x, scaler_y=scaler_y\n",
    "    )\n",
    "\n",
    "    # Build test with context borrowed from end of (train+val)\n",
    "    tv = pd.concat([train_df, val_df], axis=0).copy().sort_values(DATE_COL)\n",
    "    test_context = tv.iloc[-lookback:].copy()\n",
    "    X_test, y_test, d_test = make_supervised_multistep_with_context(\n",
    "        df_context=test_context, df_target=test_df,\n",
    "        lookback=lookback, horizon=horizon,\n",
    "        scaler_x=scaler_x, scaler_y=scaler_y\n",
    "    )\n",
    "\n",
    "    if len(X_val) == 0 or len(X_test) == 0:\n",
    "        raise ValueError(f\"Not enough samples after context stitching: val={len(X_val)} test={len(X_test)}\")\n",
    "\n",
    "    model = build_convlstm_multistep(lookback, len(FEATURE_COLS), horizon)\n",
    "\n",
    "    # Output folder\n",
    "    out_dir = os.path.join(out_root, f\"window_{lookback}\", safe_name(region), safe_name(probe))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(out_dir, \"model.h5\")\n",
    "    scaler_x_path = os.path.join(out_dir, \"scaler_x.pkl\")\n",
    "    scaler_y_path = os.path.join(out_dir, \"scaler_y.pkl\")\n",
    "\n",
    "    history_csv = os.path.join(out_dir, \"history.csv\")\n",
    "    loss_png = os.path.join(out_dir, \"loss_train_val.png\")\n",
    "\n",
    "    test_cmp_csv = os.path.join(out_dir, \"test_compare_multistep.csv\")\n",
    "    test_step1_png = os.path.join(out_dir, \"test_true_vs_pred_step1.png\")\n",
    "    test_stepH_png = os.path.join(out_dir, f\"test_true_vs_pred_step{horizon}.png\")\n",
    "\n",
    "    last_forecast_csv = os.path.join(out_dir, f\"last_date_forecast_next_{horizon}_days.csv\")\n",
    "    last_forecast_png = os.path.join(out_dir, f\"last_date_forecast_next_{horizon}_days.png\")\n",
    "\n",
    "    summary_json = os.path.join(out_dir, \"summary_metrics.json\")\n",
    "\n",
    "    cbs = [\n",
    "        callbacks.ModelCheckpoint(model_path, monitor=\"val_loss\", save_best_only=True),\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-5),\n",
    "        callbacks.CSVLogger(history_csv, append=False),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n=== TRAIN: window={lookback} horizon={horizon} | {region}/{probe} ===\")\n",
    "    print(f\"Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "    # verbose=1 prints per-epoch loss/val_loss (your requirement)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=cbs,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save scalers\n",
    "    joblib.dump(scaler_x, scaler_x_path)\n",
    "    joblib.dump(scaler_y, scaler_y_path)\n",
    "\n",
    "    # Plot loss curves\n",
    "    plot_loss(history, loss_png, title=f\"Loss: {region}/{probe} (window={lookback}, horizon={horizon})\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Test evaluation\n",
    "    # ----------------------------\n",
    "    y_pred_test_scaled = model.predict(X_test, verbose=0)          # (N, horizon)\n",
    "    y_true = inverse_y_vector(scaler_y, y_test)                    # (N, horizon)\n",
    "    y_pred = inverse_y_vector(scaler_y, y_pred_test_scaled)        # (N, horizon)\n",
    "\n",
    "    # Overall metrics on flattened values\n",
    "    overall = metrics_1d(y_true.reshape(-1), y_pred.reshape(-1))\n",
    "    print(f\"[TEST overall] MAE={overall['MAE']:.6f} RMSE={overall['RMSE']:.6f} MAPE={overall['MAPE_%']:.2f}%\")\n",
    "\n",
    "    # Save test comparison CSV with all steps\n",
    "    rows = []\n",
    "    d_test_dt = pd.to_datetime(d_test)\n",
    "    for i in range(len(d_test_dt)):\n",
    "        row = {\"forecast_start_date\": d_test_dt[i]}\n",
    "        for s in range(horizon):\n",
    "            row[f\"true_t+{s+1}\"] = y_true[i, s]\n",
    "            row[f\"pred_t+{s+1}\"] = y_pred[i, s]\n",
    "        rows.append(row)\n",
    "    pd.DataFrame(rows).to_csv(test_cmp_csv, index=False)\n",
    "\n",
    "    # Plot test curves for step 1 and step H (aligned by forecast start date)\n",
    "    plot_series(\n",
    "        dates=d_test_dt,\n",
    "        y_true=y_true[:, 0],\n",
    "        y_pred=y_pred[:, 0],\n",
    "        out_png=test_step1_png,\n",
    "        title=f\"Test True vs Pred (t+1): {region}/{probe} | window={lookback}\",\n",
    "        ylabel=\"sm_30cm (m3/m3)\"\n",
    "    )\n",
    "    plot_series(\n",
    "        dates=d_test_dt,\n",
    "        y_true=y_true[:, horizon - 1],\n",
    "        y_pred=y_pred[:, horizon - 1],\n",
    "        out_png=test_stepH_png,\n",
    "        title=f\"Test True vs Pred (t+{horizon}): {region}/{probe} | window={lookback}\",\n",
    "        ylabel=\"sm_30cm (m3/m3)\"\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Last-date multi-day forecast\n",
    "    # ----------------------------\n",
    "    df_probe_sorted = df_probe.sort_values(DATE_COL).reset_index(drop=True)\n",
    "    last_date = pd.to_datetime(df_probe_sorted[DATE_COL].iloc[-1])\n",
    "\n",
    "    last_window = df_probe_sorted[FEATURE_COLS].iloc[-lookback:].values.astype(np.float32)\n",
    "    X_last = scaler_x.transform(last_window).reshape(1, lookback, 1, len(FEATURE_COLS), 1)\n",
    "\n",
    "    y_last_scaled = model.predict(X_last, verbose=0)                # (1, horizon)\n",
    "    y_last = inverse_y_vector(scaler_y, y_last_scaled)[0]           # (horizon,)\n",
    "\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=horizon, freq=\"D\")\n",
    "    forecast_df = pd.DataFrame({\"date\": future_dates, \"pred_sm_30cm\": y_last})\n",
    "    forecast_df.to_csv(last_forecast_csv, index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(future_dates, y_last, label=\"forecast\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"sm_30cm (m3/m3)\")\n",
    "    plt.title(f\"Forecast next {horizon} days from last date: {region}/{probe} | window={lookback}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(last_forecast_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    # Save summary metrics\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"region\": region,\n",
    "            \"probe\": probe,\n",
    "            \"window\": lookback,\n",
    "            \"horizon\": horizon,\n",
    "            \"test_overall_metrics\": overall,\n",
    "            \"last_date\": str(last_date.date()),\n",
    "            \"forecast_start\": str((last_date + pd.Timedelta(days=1)).date()),\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved outputs: {out_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region=Grandvillers_Sec | probes=['Sec'] (count=1)\n",
      "\n",
      "=== TRAIN: window=14 horizon=7 | Grandvillers_Sec/Sec ===\n",
      "Samples: train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 1.4969 - val_loss: 0.7680 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7577 - val_loss: 0.7776 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6006 - val_loss: 0.7885 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.4799 - val_loss: 0.7991 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3619 - val_loss: 0.8066 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3022 - val_loss: 0.8134 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2667 - val_loss: 0.8207 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2390 - val_loss: 0.8279 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2430 - val_loss: 0.8331 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2044 - val_loss: 0.8388 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1950 - val_loss: 0.8444 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1819 - val_loss: 0.8498 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1588 - val_loss: 0.8548 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1674 - val_loss: 0.8595 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1377 - val_loss: 0.8635 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1487 - val_loss: 0.8668 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=1.243506 RMSE=1.249475 MAPE=4.76%\n",
      "Saved outputs: outputs_multistep_context\\window_14\\Grandvillers_Sec\\Sec\n",
      "\n",
      "=== TRAIN: window=30 horizon=7 | Grandvillers_Sec/Sec ===\n",
      "Samples: train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 2.0152 - val_loss: 0.7831 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8655 - val_loss: 0.7960 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5329 - val_loss: 0.8028 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5058 - val_loss: 0.8058 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4407 - val_loss: 0.8069 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3235 - val_loss: 0.8089 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2719 - val_loss: 0.8110 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2279 - val_loss: 0.8139 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2098 - val_loss: 0.8174 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2071 - val_loss: 0.8218 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1626 - val_loss: 0.8254 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1834 - val_loss: 0.8292 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1917 - val_loss: 0.8328 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1762 - val_loss: 0.8359 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2089 - val_loss: 0.8390 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1630 - val_loss: 0.8412 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=1.199662 RMSE=1.219570 MAPE=4.59%\n",
      "Saved outputs: outputs_multistep_context\\window_30\\Grandvillers_Sec\\Sec\n",
      "\n",
      "=== TRAIN: window=60 horizon=7 | Grandvillers_Sec/Sec ===\n",
      "Samples: train=14, val=11, test=12\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 4.4547 - val_loss: 0.6686 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.8837 - val_loss: 0.6795 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.5640 - val_loss: 0.6899 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1101 - val_loss: 0.6991 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7898 - val_loss: 0.7090 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5969 - val_loss: 0.7188 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4334 - val_loss: 0.7273 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3011 - val_loss: 0.7356 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2888 - val_loss: 0.7395 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3024 - val_loss: 0.7427 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2466 - val_loss: 0.7459 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3788 - val_loss: 0.7488 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2874 - val_loss: 0.7512 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2692 - val_loss: 0.7533 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2751 - val_loss: 0.7554 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2508 - val_loss: 0.7566 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=1.162688 RMSE=1.166670 MAPE=4.45%\n",
      "Saved outputs: outputs_multistep_context\\window_60\\Grandvillers_Sec\\Sec\n",
      "\n",
      "Region=Grandvillers_Canon | probes=['Canon1', 'Canon2'] (count=2)\n",
      "\n",
      "=== TRAIN: window=14 horizon=7 | Grandvillers_Canon/Canon1 ===\n",
      "Samples: train=61, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.4788 - val_loss: 0.4654 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6587 - val_loss: 0.4714 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6011 - val_loss: 0.4786 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6239 - val_loss: 0.4847 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4569 - val_loss: 0.4887 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4209 - val_loss: 0.4955 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3823 - val_loss: 0.5026 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3894 - val_loss: 0.5136 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3234 - val_loss: 0.5200 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2929 - val_loss: 0.5260 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3289 - val_loss: 0.5315 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2683 - val_loss: 0.5365 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2371 - val_loss: 0.5410 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2019 - val_loss: 0.5435 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2003 - val_loss: 0.5450 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2223 - val_loss: 0.5461 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.894502 RMSE=1.817155 MAPE=4.27%\n",
      "Saved outputs: outputs_multistep_context\\window_14\\Grandvillers_Canon\\Canon1\n",
      "\n",
      "=== TRAIN: window=30 horizon=7 | Grandvillers_Canon/Canon1 ===\n",
      "Samples: train=45, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.5446 - val_loss: 0.4405 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6930 - val_loss: 0.4527 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4447 - val_loss: 0.4652 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4330 - val_loss: 0.4777 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3833 - val_loss: 0.4871 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3621 - val_loss: 0.4934 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2985 - val_loss: 0.4983 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2413 - val_loss: 0.5026 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2394 - val_loss: 0.5059 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2183 - val_loss: 0.5104 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2109 - val_loss: 0.5153 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2551 - val_loss: 0.5209 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1739 - val_loss: 0.5261 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1840 - val_loss: 0.5310 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1609 - val_loss: 0.5353 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1643 - val_loss: 0.5384 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.824691 RMSE=1.810912 MAPE=4.01%\n",
      "Saved outputs: outputs_multistep_context\\window_30\\Grandvillers_Canon\\Canon1\n",
      "\n",
      "=== TRAIN: window=60 horizon=7 | Grandvillers_Canon/Canon1 ===\n",
      "Samples: train=15, val=11, test=12\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 2.9712 - val_loss: 0.4086 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6698 - val_loss: 0.4149 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0853 - val_loss: 0.4198 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9090 - val_loss: 0.4238 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6201 - val_loss: 0.4276 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6440 - val_loss: 0.4316 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5707 - val_loss: 0.4355 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4209 - val_loss: 0.4388 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4330 - val_loss: 0.4404 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3461 - val_loss: 0.4420 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3877 - val_loss: 0.4436 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5443 - val_loss: 0.4449 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4233 - val_loss: 0.4463 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4506 - val_loss: 0.4475 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3568 - val_loss: 0.4487 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2656 - val_loss: 0.4492 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.900312 RMSE=1.827303 MAPE=4.29%\n",
      "Saved outputs: outputs_multistep_context\\window_60\\Grandvillers_Canon\\Canon1\n",
      "\n",
      "=== TRAIN: window=14 horizon=7 | Grandvillers_Canon/Canon2 ===\n",
      "Samples: train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.4628 - val_loss: 0.4049 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.4253 - val_loss: 0.4076 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.9654 - val_loss: 0.4119 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.8401 - val_loss: 0.4169 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7521 - val_loss: 0.4213 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6567 - val_loss: 0.4245 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5435 - val_loss: 0.4269 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5514 - val_loss: 0.4275 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4288 - val_loss: 0.4273 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4367 - val_loss: 0.4271 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4023 - val_loss: 0.4274 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4172 - val_loss: 0.4285 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4393 - val_loss: 0.4304 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3391 - val_loss: 0.4330 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3820 - val_loss: 0.4360 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3395 - val_loss: 0.4375 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.322248 RMSE=0.331707 MAPE=1.23%\n",
      "Saved outputs: outputs_multistep_context\\window_14\\Grandvillers_Canon\\Canon2\n",
      "\n",
      "=== TRAIN: window=30 horizon=7 | Grandvillers_Canon/Canon2 ===\n",
      "Samples: train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 2.0771 - val_loss: 0.4000 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.3261 - val_loss: 0.3989 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9048 - val_loss: 0.4015 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7404 - val_loss: 0.4070 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7076 - val_loss: 0.4127 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6080 - val_loss: 0.4187 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5057 - val_loss: 0.4254 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4891 - val_loss: 0.4329 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4435 - val_loss: 0.4413 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4058 - val_loss: 0.4469 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3753 - val_loss: 0.4521 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3761 - val_loss: 0.4566 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3706 - val_loss: 0.4604 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3574 - val_loss: 0.4635 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3414 - val_loss: 0.4660 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3206 - val_loss: 0.4680 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3055 - val_loss: 0.4696 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.318152 RMSE=0.329831 MAPE=1.22%\n",
      "Saved outputs: outputs_multistep_context\\window_30\\Grandvillers_Canon\\Canon2\n",
      "\n",
      "=== TRAIN: window=60 horizon=7 | Grandvillers_Canon/Canon2 ===\n",
      "Samples: train=14, val=11, test=12\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 2.8590 - val_loss: 0.3676 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3632 - val_loss: 0.3756 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8847 - val_loss: 0.3831 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7693 - val_loss: 0.3892 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6878 - val_loss: 0.3938 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5086 - val_loss: 0.3972 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5116 - val_loss: 0.3994 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3564 - val_loss: 0.4007 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3068 - val_loss: 0.4016 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4035 - val_loss: 0.4024 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2621 - val_loss: 0.4031 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3120 - val_loss: 0.4038 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3291 - val_loss: 0.4044 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3189 - val_loss: 0.4050 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3248 - val_loss: 0.4055 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2855 - val_loss: 0.4058 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.359476 RMSE=0.369115 MAPE=1.37%\n",
      "Saved outputs: outputs_multistep_context\\window_60\\Grandvillers_Canon\\Canon2\n",
      "\n",
      "Region=Grandvillers_Robot_20 | probes=['Robot-20'] (count=1)\n",
      "\n",
      "=== TRAIN: window=14 horizon=7 | Grandvillers_Robot_20/Robot-20 ===\n",
      "Samples: train=59, val=10, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.2565 - val_loss: 1.3723 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.1700 - val_loss: 1.3802 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6722 - val_loss: 1.3877 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5279 - val_loss: 1.3947 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5031 - val_loss: 1.3999 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4275 - val_loss: 1.4062 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4137 - val_loss: 1.4110 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3216 - val_loss: 1.4131 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3184 - val_loss: 1.4141 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3187 - val_loss: 1.4146 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2720 - val_loss: 1.4144 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2395 - val_loss: 1.4143 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2129 - val_loss: 1.4150 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2245 - val_loss: 1.4165 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2130 - val_loss: 1.4186 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1953 - val_loss: 1.4206 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.802857 RMSE=0.803801 MAPE=2.86%\n",
      "Saved outputs: outputs_multistep_context\\window_14\\Grandvillers_Robot_20\\Robot-20\n",
      "\n",
      "=== TRAIN: window=30 horizon=7 | Grandvillers_Robot_20/Robot-20 ===\n",
      "Samples: train=43, val=10, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 2.6554 - val_loss: 1.3666 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1665 - val_loss: 1.3944 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8129 - val_loss: 1.4230 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4983 - val_loss: 1.4481 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3850 - val_loss: 1.4676 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3914 - val_loss: 1.4802 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3598 - val_loss: 1.4868 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3312 - val_loss: 1.4923 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3322 - val_loss: 1.4942 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3211 - val_loss: 1.4945 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2710 - val_loss: 1.4938 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2445 - val_loss: 1.4920 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2591 - val_loss: 1.4884 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2362 - val_loss: 1.4848 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2370 - val_loss: 1.4820 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2443 - val_loss: 1.4811 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.752034 RMSE=0.756142 MAPE=2.68%\n",
      "Saved outputs: outputs_multistep_context\\window_30\\Grandvillers_Robot_20\\Robot-20\n",
      "\n",
      "=== TRAIN: window=60 horizon=7 | Grandvillers_Robot_20/Robot-20 ===\n",
      "Samples: train=13, val=10, test=12\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 2.2005 - val_loss: 1.4389 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2474 - val_loss: 1.4537 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6584 - val_loss: 1.4676 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5781 - val_loss: 1.4799 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4932 - val_loss: 1.4897 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3065 - val_loss: 1.4970 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3269 - val_loss: 1.5011 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4429 - val_loss: 1.5036 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2767 - val_loss: 1.5041 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3821 - val_loss: 1.5037 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3446 - val_loss: 1.5027 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3561 - val_loss: 1.5014 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2022 - val_loss: 1.5000 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3329 - val_loss: 1.4988 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2963 - val_loss: 1.4980 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2630 - val_loss: 1.4979 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.756206 RMSE=0.761444 MAPE=2.70%\n",
      "Saved outputs: outputs_multistep_context\\window_60\\Grandvillers_Robot_20\\Robot-20\n",
      "\n",
      "Region=Grandvillers_Robot | probes=['Canon3', 'Robot'] (count=2)\n",
      "\n",
      "=== TRAIN: window=14 horizon=7 | Grandvillers_Robot/Canon3 ===\n",
      "Samples: train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.4085 - val_loss: 0.7151 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.2296 - val_loss: 0.7141 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8496 - val_loss: 0.7175 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.8702 - val_loss: 0.7235 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6560 - val_loss: 0.7307 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6364 - val_loss: 0.7373 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5897 - val_loss: 0.7420 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4952 - val_loss: 0.7446 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4427 - val_loss: 0.7471 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.4394 - val_loss: 0.7495 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3801 - val_loss: 0.7526 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3923 - val_loss: 0.7575 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3544 - val_loss: 0.7627 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3204 - val_loss: 0.7676 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3082 - val_loss: 0.7725 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3089 - val_loss: 0.7772 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2373 - val_loss: 0.7803 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.096245 RMSE=0.114617 MAPE=0.35%\n",
      "Saved outputs: outputs_multistep_context\\window_14\\Grandvillers_Robot\\Canon3\n",
      "\n",
      "=== TRAIN: window=30 horizon=7 | Grandvillers_Robot/Canon3 ===\n",
      "Samples: train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.9924 - val_loss: 0.6350 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.1345 - val_loss: 0.6458 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.9398 - val_loss: 0.6560 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.6832 - val_loss: 0.6663 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5549 - val_loss: 0.6745 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5261 - val_loss: 0.6800 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3460 - val_loss: 0.6849 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3690 - val_loss: 0.6909 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3499 - val_loss: 0.6950 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3268 - val_loss: 0.6995 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2677 - val_loss: 0.7042 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2466 - val_loss: 0.7089 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3530 - val_loss: 0.7138 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2700 - val_loss: 0.7182 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2839 - val_loss: 0.7224 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2831 - val_loss: 0.7256 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.132125 RMSE=0.147670 MAPE=0.48%\n",
      "Saved outputs: outputs_multistep_context\\window_30\\Grandvillers_Robot\\Canon3\n",
      "\n",
      "=== TRAIN: window=60 horizon=7 | Grandvillers_Robot/Canon3 ===\n",
      "Samples: train=14, val=11, test=12\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 2.4583 - val_loss: 0.6116 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.8642 - val_loss: 0.6187 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2648 - val_loss: 0.6286 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9916 - val_loss: 0.6384 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2004 - val_loss: 0.6471 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6065 - val_loss: 0.6545 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6455 - val_loss: 0.6603 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6770 - val_loss: 0.6651 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4410 - val_loss: 0.6673 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4714 - val_loss: 0.6691 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5623 - val_loss: 0.6710 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6319 - val_loss: 0.6732 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3627 - val_loss: 0.6756 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4717 - val_loss: 0.6781 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3566 - val_loss: 0.6808 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3965 - val_loss: 0.6825 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.143938 RMSE=0.163498 MAPE=0.53%\n",
      "Saved outputs: outputs_multistep_context\\window_60\\Grandvillers_Robot\\Canon3\n",
      "\n",
      "=== TRAIN: window=14 horizon=7 | Grandvillers_Robot/Robot ===\n",
      "Samples: train=60, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.7926 - val_loss: 0.3787 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.5272 - val_loss: 0.3815 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.0933 - val_loss: 0.3844 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.9147 - val_loss: 0.3863 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8331 - val_loss: 0.3877 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.7414 - val_loss: 0.3880 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6203 - val_loss: 0.3860 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5900 - val_loss: 0.3833 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.4715 - val_loss: 0.3814 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4809 - val_loss: 0.3799 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4686 - val_loss: 0.3788 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4137 - val_loss: 0.3782 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3997 - val_loss: 0.3780 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3992 - val_loss: 0.3782 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4105 - val_loss: 0.3789 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3984 - val_loss: 0.3801 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4019 - val_loss: 0.3814 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3504 - val_loss: 0.3824 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3055 - val_loss: 0.3833 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3628 - val_loss: 0.3838 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3307 - val_loss: 0.3836 - lr: 2.5000e-04\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3169 - val_loss: 0.3834 - lr: 2.5000e-04\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2904 - val_loss: 0.3833 - lr: 2.5000e-04\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3083 - val_loss: 0.3831 - lr: 2.5000e-04\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3281 - val_loss: 0.3828 - lr: 2.5000e-04\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.3199 - val_loss: 0.3827 - lr: 2.5000e-04\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2965 - val_loss: 0.3826 - lr: 2.5000e-04\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3049 - val_loss: 0.3821 - lr: 1.2500e-04\n",
      "[TEST overall] MAE=0.187319 RMSE=0.216190 MAPE=0.74%\n",
      "Saved outputs: outputs_multistep_context\\window_14\\Grandvillers_Robot\\Robot\n",
      "\n",
      "=== TRAIN: window=30 horizon=7 | Grandvillers_Robot/Robot ===\n",
      "Samples: train=44, val=11, test=12\n",
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.0160 - val_loss: 0.4528 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0107 - val_loss: 0.4636 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.6855 - val_loss: 0.4696 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5650 - val_loss: 0.4734 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5676 - val_loss: 0.4760 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4345 - val_loss: 0.4792 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4585 - val_loss: 0.4839 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3829 - val_loss: 0.4885 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3154 - val_loss: 0.4918 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3204 - val_loss: 0.4956 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3045 - val_loss: 0.4993 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2775 - val_loss: 0.5028 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3110 - val_loss: 0.5059 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2829 - val_loss: 0.5087 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2771 - val_loss: 0.5117 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2474 - val_loss: 0.5139 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.187755 RMSE=0.224438 MAPE=0.74%\n",
      "Saved outputs: outputs_multistep_context\\window_30\\Grandvillers_Robot\\Robot\n",
      "\n",
      "=== TRAIN: window=60 horizon=7 | Grandvillers_Robot/Robot ===\n",
      "Samples: train=14, val=11, test=12\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 2.9888 - val_loss: 0.4322 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.7453 - val_loss: 0.4502 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3918 - val_loss: 0.4613 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9288 - val_loss: 0.4681 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7488 - val_loss: 0.4728 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4489 - val_loss: 0.4766 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4232 - val_loss: 0.4787 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4541 - val_loss: 0.4801 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3893 - val_loss: 0.4805 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3427 - val_loss: 0.4807 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3464 - val_loss: 0.4809 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2626 - val_loss: 0.4810 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2998 - val_loss: 0.4808 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2501 - val_loss: 0.4806 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3239 - val_loss: 0.4804 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2831 - val_loss: 0.4802 - lr: 2.5000e-04\n",
      "[TEST overall] MAE=0.272576 RMSE=0.296650 MAPE=1.08%\n",
      "Saved outputs: outputs_multistep_context\\window_60\\Grandvillers_Robot\\Robot\n",
      "\n",
      "All unique probe_name values across 4 CSV:\n",
      "['Canon1', 'Canon2', 'Canon3', 'Robot', 'Robot-20', 'Sec']\n",
      "Total unique probe_name: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Main: run all windows for all CSVs and all probes\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    REGION_CSVS: Dict[str, str] = {\n",
    "        \"Grandvillers_Sec\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers_Sec.csv\",\n",
    "        \"Grandvillers_Canon\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Canon.csv\",\n",
    "        \"Grandvillers_Robot_20\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot-20.csv\",\n",
    "        \"Grandvillers_Robot\": r\"D:\\UV Projet\\Soil Moisture\\Grandvillers-Robot.csv\",\n",
    "    }\n",
    "\n",
    "    OUT_ROOT = \"outputs_multistep_context\"\n",
    "\n",
    "    all_probes = set()\n",
    "\n",
    "    for region, path in REGION_CSVS.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = load_csv(path)\n",
    "\n",
    "        probes = sorted(df[PROBE_COL].dropna().unique().tolist())\n",
    "        all_probes.update(probes)\n",
    "\n",
    "        print(f\"\\nRegion={region} | probes={probes} (count={len(probes)})\")\n",
    "\n",
    "        for probe in probes:\n",
    "            df_probe = df[df[PROBE_COL] == probe].copy()\n",
    "\n",
    "            for w in WINDOWS:\n",
    "                try:\n",
    "                    run_one_setting(region, probe, df_probe, lookback=w, horizon=HORIZON, out_root=OUT_ROOT)\n",
    "                except Exception as e:\n",
    "                    print(f\"[SKIP] {region}/{probe} window={w} horizon={HORIZON} -> {e}\")\n",
    "\n",
    "    print(\"\\nAll unique probe_name values across 4 CSV:\")\n",
    "    print(sorted(list(all_probes)))\n",
    "    print(f\"Total unique probe_name: {len(all_probes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We evaluated ConvLSTM models with different temporal window sizes (14, 30, and 60 days) for multi-step soil moisture forecasting at 30 cm depth. Across all window configurations, the models consistently exhibited overfitting behavior, characterized by a rapid decrease in training loss while validation loss remained stable or increased. This phenomenon is primarily attributed to the limited temporal sample size, the high autocorrelation of soil moisture time series, and the relatively high model capacity of ConvLSTM compared to the available data. Increasing the temporal window did not improve generalization performance, suggesting that longer historical contexts introduce redundant information rather than additional predictive power. Overall, smaller temporal windows (e.g., 14 days) provided more stable test performance and were therefore considered more suitable for this dataset.\n",
    "\n",
    "window=14/30/60 全部过拟合\n",
    "\n",
    "原因是：\n",
    "\n",
    "数据太短\n",
    "\n",
    "土壤水分高度自相关\n",
    "\n",
    "ConvLSTM 太强\n",
    "\n",
    "window 变大没有带来额外信息\n",
    "\n",
    "小 window 反而更稳、更合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 ： 改成小 window，filters = 8 or 16, Dropout(0.2)。在训练时，随机“关闭”一部分神经元，防止模型过度依赖某些特征"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndvi_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
